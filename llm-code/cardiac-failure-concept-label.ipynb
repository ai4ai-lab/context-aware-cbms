{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0HKuIqz6mak"
      },
      "source": [
        "# Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8VbkI4zR6MOl",
        "outputId": "320ef555-2182-457b-e664-8cfbff8494d1"
      },
      "outputs": [],
      "source": [
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J83CiHJ28ywv"
      },
      "source": [
        "Inside terminal run the following:\n",
        "\n",
        "```\n",
        "ollama serve &\n",
        "ollama run llama3\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iDly6F4x70cy",
        "outputId": "d785ef61-6d26-4884-dd7c-46e94c3535ad"
      },
      "outputs": [],
      "source": [
        "!pip install langchain_community"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBKxocof9Hyz",
        "outputId": "c51a7104-62d9-4245-f249-705e57caf4bf"
      },
      "outputs": [],
      "source": [
        "# Load Google Drive because it stores /content/drive/My Drive/ards-cohort-notes/ards-cohort-notes.csv\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYn-iYTa6ycW"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lMw0VBWN9DkQ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import time\n",
        "import random\n",
        "import csv\n",
        "from langchain_community.llms import Ollama\n",
        "from langchain.callbacks.manager import CallbackManager\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "from langchain_core.prompts import PromptTemplate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWQhLQDK638b"
      },
      "source": [
        "# Functions to load data, specify LLM prompt, and perform LLM inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PYC4wFzh64z-"
      },
      "outputs": [],
      "source": [
        "def load_data(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    df.fillna('', inplace=True)\n",
        "    return df\n",
        "\n",
        "def select_random_start(num_rows, min_rows=15):\n",
        "    if num_rows < min_rows:\n",
        "        raise ValueError(f\"The dataset must contain at least {min_rows} rows to process.\")\n",
        "    return random.randint(0, num_rows - min_rows)\n",
        "\n",
        "def create_prompt_template(task):\n",
        "    return PromptTemplate(\n",
        "        template=(\n",
        "            f\"Context: You are a clinician receiving chunks of clinical text for patients in an ICU. Please do the reviewing as quickly as possible.\\n\"\n",
        "            f\"Task: Determine if the patient suffered from {task}.\\n\"\n",
        "            f\"Instructions: Answer with 'Yes' or 'No'. If there is not enough information, answer 'No'.\\n\"\n",
        "            f\"Text:\\n{{text}}\\n\\n\"\n",
        "            f\"Query: Does the chunk of text mention that the patient suffered from {task}? Answer strictly in 'Yes' or 'No'.\"\n",
        "        ),\n",
        "        input_variables=[\"text\"]\n",
        "    )\n",
        "\n",
        "def chunk_text(text, chunk_size, overlap):\n",
        "    start = 0\n",
        "    chunks = []\n",
        "    while start < len(text):\n",
        "        end = start + chunk_size\n",
        "        chunks.append(text[start:end])\n",
        "        start += chunk_size - overlap\n",
        "    return chunks\n",
        "\n",
        "def check_for_condition(text, llm, prompt_template, chunk_size, chunk_overlap):\n",
        "    chunks = chunk_text(text, chunk_size, chunk_overlap)\n",
        "    results = []\n",
        "    for chunk in chunks:\n",
        "        prompt = prompt_template.format(text=chunk)\n",
        "        try:\n",
        "            response = llm.invoke(prompt)\n",
        "            results.append(response.strip())\n",
        "        except Exception as e:\n",
        "            results.append(f\"Error invoking model: {e}\")\n",
        "    condition_mentions = [res for res in results if \"Yes\" in res]\n",
        "    if condition_mentions:\n",
        "        return \"Yes\", condition_mentions[0], len(text)\n",
        "    else:\n",
        "        return \"No\", results[0] if results else \"No sufficient data\", len(text)\n",
        "\n",
        "def process_patients(df, start_index, num_patients, llm, prompt_template_cardiac, prompt_template_discharge, chunk_size, chunk_overlap, output_csv_file, progress_report_file):\n",
        "    processing_time = []\n",
        "    with open(output_csv_file, 'a', newline='') as csvfile, open(progress_report_file, 'a') as report_file:\n",
        "        fieldnames = ['hadm_id', 'text_length', 'cardiac_failure_detected', 'time_taken']\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "\n",
        "        if csvfile.tell() == 0:\n",
        "            writer.writeheader()\n",
        "\n",
        "        for i in range(start_index, start_index + num_patients):\n",
        "            current_hadm_id = df['hadm_id'].values[i]\n",
        "            start_time = time.time()\n",
        "            data = df[df['hadm_id'] == current_hadm_id]\n",
        "            if data.empty:\n",
        "                result = f\"No data found for hadm_id: {current_hadm_id}\"\n",
        "            else:\n",
        "                ecd_combined_reports = data['ecd_combined_reports'].values[0]\n",
        "                cardiac_failure_result, explanation, ecd_combined_reports_length = check_for_condition(ecd_combined_reports, llm, prompt_template_cardiac, chunk_size, chunk_overlap)\n",
        "\n",
        "                if cardiac_failure_result == \"No\":\n",
        "                    discharge_text = data['discharge_text'].values[0]\n",
        "                    cardiac_failure_result, explanation, discharge_text_length = check_for_condition(discharge_text, llm, prompt_template_discharge, chunk_size, chunk_overlap)\n",
        "\n",
        "                end_time = time.time()\n",
        "                elapsed_time = end_time - start_time\n",
        "                processing_time.append(elapsed_time)\n",
        "\n",
        "                writer.writerow({\n",
        "                    'hadm_id': current_hadm_id,\n",
        "                    'text_length': max(ecd_combined_reports_length, len(discharge_text)),\n",
        "                    'cardiac_failure_detected': cardiac_failure_result,\n",
        "                    'time_taken': round(elapsed_time)\n",
        "                })\n",
        "                csvfile.flush()\n",
        "\n",
        "                report_file.write(f\"Patient Number: {i}, HADM ID: {current_hadm_id}, Text Length: {max(ecd_combined_reports_length, len(discharge_text))}, Cardiac Failure Detected: {cardiac_failure_result}, Time Taken: {round(elapsed_time)}\\n\")\n",
        "                report_file.flush()\n",
        "\n",
        "                print(f\"Processed Patient Number {i}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eq5VseRR7Dfe"
      },
      "source": [
        "# Main (calls all the functions above)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Set `file_path` to the directory containing the cohort notes for all patients."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ahusYxXm9kK8",
        "outputId": "9e2bef93-9639-48ab-dd4d-cce05e6457ce"
      },
      "outputs": [],
      "source": [
        "def main(file_path, model_name, chunk_size, chunk_overlap, output_csv_file, progress_report_file, num_patients):\n",
        "    df = load_data(file_path)\n",
        "    # start_index = select_random_start(len(df))\n",
        "    start_index = 0\n",
        "    prompt_template_cardiac = create_prompt_template(\"cardiac failure\")\n",
        "    prompt_template_discharge = create_prompt_template(\"cardiac failure\")\n",
        "    llm = Ollama(model=model_name, callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]))\n",
        "    process_patients(df, start_index, num_patients, llm, prompt_template_cardiac, prompt_template_discharge, chunk_size, chunk_overlap, output_csv_file, progress_report_file)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main(\n",
        "        file_path='/content/drive/My Drive/ards-cohort-notes/ards-cohort-notes.csv',\n",
        "        model_name=\"llama3\",\n",
        "        chunk_size=4096,\n",
        "        chunk_overlap=100,\n",
        "        output_csv_file='cardiac-failure-concept-label.csv',\n",
        "        progress_report_file='cardiac-failure-concept-label.txt',\n",
        "        num_patients = 1953\n",
        "    )"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
