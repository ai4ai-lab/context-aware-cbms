{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11286599,"sourceType":"datasetVersion","datasetId":5679533}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import StepLR\nfrom torch.utils.data import DataLoader, Dataset\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score, roc_auc_score, mean_squared_error, mean_absolute_error, r2_score\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-29T06:13:38.467806Z","iopub.execute_input":"2025-05-29T06:13:38.468037Z","iopub.status.idle":"2025-05-29T06:13:45.258438Z","shell.execute_reply.started":"2025-05-29T06:13:38.468017Z","shell.execute_reply":"2025-05-29T06:13:45.257456Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/mimic-ards/mlhc-ards-cohort-data.csv\")\n\nX_cont = df[['max_norepinephrine_equiv', 'avg_norepinephrine_equiv', 'sofa_cardiovascular_avg_meanbp','sofa_cardiovascular_avg_rate_norepinephrine', 'sofa_respiration_avg_pao2fio2ratio', 'sofa_renal_avg_urineoutput', 'sofa_renal_avg_creatinine', 'sofa_cns_avg_gcs', 'first24hr_cardiovascular_rate_norepinephrine', 'first24hr_cardiovascular_meanbp', 'first24hr_respiration_pao2fio2ratio', 'first24hr_renal_urineoutput','first24hr_renal_creatinine', 'first24hr_cns_gcs', 'sofa_cardiovascular_worst_meanbp', 'sofa_cardiovascular_worst_rate_norepinephrine', 'sofa_respiration_worst_pao2fio2ratio', 'sofa_renal_worst_urineoutput', 'sofa_renal_worst_creatinine', 'sofa_cns_worst_gcs','mech_vent_duration_minutes']]\n\nX_bin = df[['other_respiratory_diseases', 'lung_diseases_due_to_external_agents', 'chronic_lower_respiratory_diseases', 'acute_lower_respiratory_infections', 'influenza_pneumonia', 'upper_respiratory_infections']]\n\nC_cont = df[['c_sofa_avg_cardiovascular', 'c_sofa_avg_respiration', 'c_sofa_avg_renal', 'c_sofa_avg_cns', 'c_first24hr_sofa_max_cardiovascular', 'c_first24hr_sofa_max_respiration', 'c_first24hr_sofa_max_renal', 'c_first24hr_sofa_max_cns', 'c_sofa_max_cardiovascular', 'c_sofa_max_respiration', 'c_sofa_max_renal', 'c_sofa_max_cns']]\n\nC_bin = df[['c_svr_resp_comorbidity', 'c_mod_resp_comorbidity']]\n\nLLM_C = df[['ards_detected','aspiration_detected','bilateral_infiltrates_detected', 'cardiac_arrest_detected', 'cardiac_failure_detected', 'pancreatitis_detected','pneumonia_detected','trali_detected']]\n\nY = df['ARDS_DIAGNOSIS']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T06:13:45.260902Z","iopub.execute_input":"2025-05-29T06:13:45.261418Z","iopub.status.idle":"2025-05-29T06:13:45.345146Z","shell.execute_reply.started":"2025-05-29T06:13:45.261391Z","shell.execute_reply":"2025-05-29T06:13:45.343986Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\n\nclass MIMICDataProcessor:\n    def __init__(self, file_path, batch_size=64):\n        self.file_path = file_path\n        self.batch_size = batch_size\n        self.x_scaler = MinMaxScaler()\n        self.c_scaler = MinMaxScaler()\n\n        # Load and clean data\n        self.df = pd.read_csv(file_path)\n\n        self.X_cont = self.df[['max_norepinephrine_equiv', 'avg_norepinephrine_equiv', 'sofa_cardiovascular_avg_meanbp',\n                          'sofa_cardiovascular_avg_rate_norepinephrine', 'sofa_respiration_avg_pao2fio2ratio',\n                          'sofa_renal_avg_urineoutput', 'sofa_renal_avg_creatinine', 'sofa_cns_avg_gcs', 'first24hr_cardiovascular_meanbp',\n                          'sofa_cardiovascular_worst_meanbp',\n                          'sofa_cardiovascular_worst_rate_norepinephrine', 'sofa_respiration_worst_pao2fio2ratio',\n                          'sofa_renal_worst_urineoutput', 'sofa_cns_worst_gcs',\n                          'mech_vent_duration_minutes']]\n\n        self.X_bin = self.df[['other_respiratory_diseases', 'lung_diseases_due_to_external_agents',\n                         'chronic_lower_respiratory_diseases', 'acute_lower_respiratory_infections',\n                         'influenza_pneumonia', 'upper_respiratory_infections']]\n\n        self.C_cont = self.df[['c_sofa_avg_cardiovascular', 'c_sofa_avg_respiration', 'c_sofa_avg_renal', 'c_sofa_avg_cns',\n                          'c_first24hr_sofa_max_cardiovascular', 'c_first24hr_sofa_max_respiration',\n                          'c_first24hr_sofa_max_renal', 'c_first24hr_sofa_max_cns', 'c_sofa_max_cardiovascular',\n                          'c_sofa_max_respiration', 'c_sofa_max_renal', 'c_sofa_max_cns']]\n\n        self.C_bin = self.df[['c_svr_resp_comorbidity', 'c_mod_resp_comorbidity']]\n\n        self.LLM_C = self.df[['ards_detected', 'aspiration_detected', 'bilateral_infiltrates_detected',\n                         'cardiac_arrest_detected', 'cardiac_failure_detected', 'pancreatitis_detected',\n                         'pneumonia_detected', 'trali_detected']]\n\n        self.Y = self.df['ARDS_DIAGNOSIS']\n        hospital = self.df[['hadm_id']]\n\n        # Split raw data into train/val/test sets\n        X_cont_temp, X_cont_test, X_bin_temp, X_bin_test, C_cont_temp, C_cont_test, C_bin_temp, C_bin_test, \\\n        LLM_C_temp, LLM_C_test, Y_temp, Y_test, hospital_train, hospital_test = train_test_split(\n            self.X_cont, self.X_bin, self.C_cont, self.C_bin, self.LLM_C, self.Y, hospital, test_size=0.20, random_state=42)\n\n        X_cont_train, X_cont_val, X_bin_train, X_bin_val, C_cont_train, C_cont_val, C_bin_train, C_bin_val, \\\n        LLM_C_train, LLM_C_val, Y_train, Y_val = train_test_split(\n            X_cont_temp, X_bin_temp, C_cont_temp, C_bin_temp, LLM_C_temp, Y_temp, test_size=0.25, random_state=42)\n\n        # Impute using median from training data only\n        cont_cols_x = self.X_cont.columns.tolist()\n        cont_cols_c = self.C_cont.columns.tolist()\n        \n        # Compute medians from training data\n        x_medians = X_cont_train.median()\n        c_medians = C_cont_train.median()\n        \n        # Apply to all splits\n        X_cont_train = X_cont_train.fillna(x_medians)\n        X_cont_val = X_cont_val.fillna(x_medians)\n        X_cont_test = X_cont_test.fillna(x_medians)\n        \n        C_cont_train = C_cont_train.fillna(c_medians)\n        C_cont_val = C_cont_val.fillna(c_medians)\n        C_cont_test = C_cont_test.fillna(c_medians)\n\n        # Fit scalers on train data only\n        self.x_scaler.fit(X_cont_train)\n        self.c_scaler.fit(C_cont_train)\n\n        # Transform all splits\n        X_train_scaled = self.x_scaler.transform(X_cont_train)\n        X_val_scaled = self.x_scaler.transform(X_cont_val)\n        X_test_scaled = self.x_scaler.transform(X_cont_test)\n\n        X_train_full = np.concatenate([X_train_scaled, X_bin_train.values.astype(float)], axis=1)\n        X_val_full = np.concatenate([X_val_scaled, X_bin_val.values.astype(float)], axis=1)\n        X_test_full = np.concatenate([X_test_scaled, X_bin_test.values.astype(float)], axis=1)\n\n        C_train_scaled = self.c_scaler.transform(C_cont_train)\n        C_val_scaled = self.c_scaler.transform(C_cont_val)\n        C_test_scaled = self.c_scaler.transform(C_cont_test)\n\n        C_train_full = np.concatenate([C_train_scaled, C_bin_train.values.astype(float)], axis=1)\n        C_val_full = np.concatenate([C_val_scaled, C_bin_val.values.astype(float)], axis=1)\n        C_test_full = np.concatenate([C_test_scaled, C_bin_test.values.astype(float)], axis=1)\n\n        # Convert to tensors\n        self.X_tensor_scaled_train = torch.tensor(X_train_full, dtype=torch.float32)\n        self.X_tensor_scaled_val = torch.tensor(X_val_full, dtype=torch.float32)\n        self.X_tensor_scaled_test = torch.tensor(X_test_full, dtype=torch.float32)\n\n        self.C_tensor_train = torch.tensor(C_train_full, dtype=torch.float32)\n        self.C_tensor_val = torch.tensor(C_val_full, dtype=torch.float32)\n        self.C_tensor_test = torch.tensor(C_test_full, dtype=torch.float32)\n\n        self.LLM_C_tensor_train = torch.tensor(LLM_C_train.values, dtype=torch.float32)\n        self.LLM_C_tensor_val = torch.tensor(LLM_C_val.values, dtype=torch.float32)\n        self.LLM_C_tensor_test = torch.tensor(LLM_C_test.values, dtype=torch.float32)\n\n        self.Y_tensor_train = torch.tensor(Y_train.values, dtype=torch.float32)\n        self.Y_tensor_val = torch.tensor(Y_val.values, dtype=torch.float32)\n        self.Y_tensor_test = torch.tensor(Y_test.values, dtype=torch.float32)\n\n        self.hospital_test = hospital_test\n\n    def create_dataloaders(self):\n        train_dataset = self.MIMICDataset(self.X_tensor_scaled_train, self.C_tensor_train,\n                                          self.LLM_C_tensor_train, self.Y_tensor_train)\n        val_dataset = self.MIMICDataset(self.X_tensor_scaled_val, self.C_tensor_val,\n                                        self.LLM_C_tensor_val, self.Y_tensor_val)\n        test_dataset = self.MIMICDataset(self.X_tensor_scaled_test, self.C_tensor_test,\n                                         self.LLM_C_tensor_test, self.Y_tensor_test)\n\n        train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n        val_loader = DataLoader(val_dataset, batch_size=self.batch_size, shuffle=False)\n        test_loader = DataLoader(test_dataset, batch_size=self.batch_size, shuffle=False)\n\n        return train_loader, val_loader, test_loader, self.hospital_test\n\n    def get_features(self):\n        return self.df[['max_norepinephrine_equiv', 'avg_norepinephrine_equiv', 'sofa_cardiovascular_avg_meanbp',\n                        'sofa_cardiovascular_avg_rate_norepinephrine', 'sofa_respiration_avg_pao2fio2ratio',\n                        'sofa_renal_avg_urineoutput', 'sofa_renal_avg_creatinine', 'sofa_cns_avg_gcs',\n                        'first24hr_cardiovascular_rate_norepinephrine', 'first24hr_cardiovascular_meanbp',\n                        'first24hr_respiration_pao2fio2ratio', 'first24hr_renal_urineoutput',\n                        'first24hr_renal_creatinine', 'first24hr_cns_gcs', 'sofa_cardiovascular_worst_meanbp',\n                        'sofa_cardiovascular_worst_rate_norepinephrine', 'sofa_respiration_worst_pao2fio2ratio',\n                        'sofa_renal_worst_urineoutput', 'sofa_renal_worst_creatinine', 'sofa_cns_worst_gcs',\n                        'mech_vent_duration_minutes']].columns.tolist()\n\n    def get_vanilla_concepts(self):\n        return self.df[['c_sofa_avg_cardiovascular', 'c_sofa_avg_respiration', 'c_sofa_avg_renal', 'c_sofa_avg_cns',\n                        'c_first24hr_sofa_max_cardiovascular', 'c_first24hr_sofa_max_respiration',\n                        'c_first24hr_sofa_max_renal', 'c_first24hr_sofa_max_cns', 'c_sofa_max_cardiovascular',\n                        'c_sofa_max_respiration', 'c_sofa_max_renal', 'c_sofa_max_cns',\n                        'c_svr_resp_comorbidity', 'c_mod_resp_comorbidity']].columns.tolist()\n\n    def get_llm_concepts(self):\n        return self.df[['ards_detected', 'aspiration_detected', 'bilateral_infiltrates_detected',\n                        'cardiac_arrest_detected', 'cardiac_failure_detected', 'pancreatitis_detected',\n                        'pneumonia_detected', 'trali_detected']].columns.tolist()\n\n    class MIMICDataset(Dataset):\n        def __init__(self, x, c, llm_c, y):\n            self.x = x\n            self.c = c\n            self.llm_c = llm_c\n            self.y = y\n\n        def __len__(self):\n            return len(self.y)\n\n        def __getitem__(self, idx):\n            return self.x[idx], self.c[idx], self.llm_c[idx], self.y[idx]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T06:13:45.346344Z","iopub.execute_input":"2025-05-29T06:13:45.346699Z","iopub.status.idle":"2025-05-29T06:13:45.370725Z","shell.execute_reply.started":"2025-05-29T06:13:45.346668Z","shell.execute_reply":"2025-05-29T06:13:45.369592Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"class MultiLabelNN1(nn.Module):\n    def __init__(self, num_features, num_binary_concepts, num_continuous_concepts, num_labels):\n        super(MultiLabelNN1, self).__init__()\n        \n        self.layer1 = nn.Linear(num_features, num_binary_concepts, bias=False)\n        self.layer2 = nn.Linear(num_features, num_continuous_concepts, bias=False)\n        self.layer3 = nn.Linear(num_binary_concepts, num_labels, bias=False)\n        self.layer4 = nn.Linear(num_continuous_concepts, num_labels, bias=False)\n        \n    def forward(self, x):\n        binary_c = self.layer1(x)\n        continuous_c = self.layer2(x)\n        \n        binary_c = torch.sigmoid(binary_c)\n        \n        y_pred = torch.sigmoid(self.layer3(binary_c)+self.layer4(continuous_c))\n        return y_pred, binary_c, continuous_c\n\nclass MultiLabelNN2(nn.Module):\n    def __init__(self, num_features, num_binary_concepts, num_continuous_concepts, num_llm_concepts, num_labels):\n        super(MultiLabelNN2, self).__init__()\n        \n        self.layer1 = nn.Linear(num_features, num_binary_concepts, bias=False)\n        self.layer2 = nn.Linear(num_features, num_continuous_concepts, bias=False)\n        self.layer3 = nn.Linear(num_binary_concepts, num_labels, bias=False)\n        self.layer4 = nn.Linear(num_continuous_concepts, num_labels, bias=False)\n        self.layer5 = nn.Linear(num_llm_concepts, num_labels, bias=False)\n\n    def forward(self, x, llm_c):\n        binary_c = self.layer1(x)\n        continuous_c = self.layer2(x)\n        \n        binary_c = torch.sigmoid(binary_c)\n        \n        y_pred = torch.sigmoid(self.layer3(binary_c)+self.layer4(continuous_c)+self.layer5(llm_c))\n        return y_pred, binary_c, continuous_c","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T06:13:45.371642Z","iopub.execute_input":"2025-05-29T06:13:45.372027Z","iopub.status.idle":"2025-05-29T06:13:45.398368Z","shell.execute_reply.started":"2025-05-29T06:13:45.371973Z","shell.execute_reply":"2025-05-29T06:13:45.397221Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"from copy import deepcopy\n\nclass EarlyStopper:\n    def __init__(self, patience=25):\n        self.patience = patience\n        self.counter = 0\n        self.best_score = None\n        self.best_model = None\n\n    def should_stop(self, score, model):\n        if self.best_score is None or score > self.best_score:\n            self.best_score = score\n            self.best_model = deepcopy(model)\n            self.counter = 0\n            return False\n        else:\n            self.counter += 1\n            return self.counter >= self.patience\n\ncriterion = nn.BCELoss()\n\ndef concept_loss(binary_c_pred, continuous_c_pred, vanilla_c, binary_concept_idx):\n\n    bce_loss = nn.BCELoss()\n    mse_loss = nn.MSELoss()\n\n    # print(binary_c_pred.shape,vanilla_c.shape,vanilla_c[:, binary_concept_idx].shape)\n    binary_loss = bce_loss(binary_c_pred,vanilla_c[:, binary_concept_idx]) if binary_concept_idx else 0\n\n    continuous_idx = [i for i in range(vanilla_c.shape[1]) if i not in binary_concept_idx]\n    continuous_loss = mse_loss(continuous_c_pred,vanilla_c[:, continuous_idx]) if continuous_idx else 0\n\n    return binary_loss + continuous_loss\n\ndef train_combined_model(model, x_size, vanilla_c_size, llm_c_size, y_size, learning_rate, epochs, train_loader, val_loader, binary_concept_idx, weight_decay=0.01):\n    \n    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n    #scheduler = StepLR(optimizer, step_size=25, gamma=0.1)\n    stopper = EarlyStopper(30)\n    \n    epochs_count = []\n    binary_c_predictions, continuous_c_predictions, label_predictions = [], [], []\n    binary_c_val_predictions, continuous_c_val_predictions, label_val_predictions = [], [], []\n    \n    ground_truth_val_c, ground_truth_val_y = [], []\n\n    for epoch in range(epochs):\n        #print(f\"Epoch {epoch+1}/{epochs}\")  \n        epochs_count.append(epoch)\n\n        model.train()\n        running_loss = 0.0\n\n        for i, batch in enumerate(train_loader):\n            x, vanilla_c, llm_c, y = batch\n            x, vanilla_c, llm_c, y = x.to(device), vanilla_c.to(device), llm_c.to(device), y.to(device)\n\n            optimizer.zero_grad()\n            y_pred, binary_c_pred, continuous_c_pred = model(x, llm_c)\n\n            binary_c_predictions.append(binary_c_pred.detach().cpu().numpy())\n            continuous_c_predictions.append(continuous_c_pred.detach().cpu().numpy())\n            label_predictions.append(y_pred.detach().cpu().numpy())\n            \n            c_loss = concept_loss(binary_c_pred, continuous_c_pred, vanilla_c, binary_concept_idx)\n            y_loss = criterion(y_pred, y.unsqueeze(1).float())\n\n            loss = y_loss + 0.5*c_loss\n            \n            loss.backward()\n            optimizer.step()\n\n        model.eval()\n        \n        val_loss = 0.0\n\n        with torch.no_grad():\n            for x, vanilla_c, llm_c, y in val_loader:\n                x, vanilla_c, llm_c, y = x.to(device), vanilla_c.to(device), llm_c.to(device), y.to(device)\n\n                ground_truth_val_c.append(vanilla_c.cpu())\n                ground_truth_val_y.append(y.cpu())\n\n                y_pred, binary_c_pred, continuous_c_pred = model(x, llm_c)\n\n                binary_c_val_predictions.append(binary_c_pred.detach().cpu().numpy())\n                continuous_c_val_predictions.append(continuous_c_pred.detach().cpu().numpy())\n                label_val_predictions.append(y_pred.detach().cpu().numpy())\n\n                c_loss = concept_loss(binary_c_pred, continuous_c_pred, vanilla_c, binary_concept_idx)\n                y_loss = criterion(y_pred, y.unsqueeze(1).float())\n\n                val_loss += y_loss + 0.5*c_loss\n\n        if stopper.should_stop(val_loss,model):\n            print(\"Done\")\n            # break\n        #scheduler.step()\n\n    return model, binary_c_predictions, continuous_c_predictions, label_predictions, binary_c_val_predictions, continuous_c_val_predictions, label_val_predictions, ground_truth_val_c, ground_truth_val_y\n\ndef train(model, x_size, c_size, y_size, learning_rate, weight_decay, epochs, train_loader, val_loader, binary_concept_idx):\n    \n    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n    #scheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n    stopper = EarlyStopper(30)\n    \n    epochs_count = []\n    binary_c_predictions, continuous_c_predictions, label_predictions = [], [], []\n    binary_c_val_predictions, continuous_c_val_predictions, label_val_predictions = [], [], []\n    \n    ground_truth_val_c, ground_truth_val_y = [], []\n\n    for epoch in range(epochs):\n        #print(f\"Epoch {epoch+1}/{epochs}\")  # Print current epoch\n        epochs_count.append(epoch)\n\n        # Training Loop\n        model.train()\n\n        for i, batch in enumerate(train_loader):\n            x, c,_, y = batch\n            x, c, y = x.to(device), c.to(device), y.to(device)\n\n            optimizer.zero_grad()\n            \n            y_pred, binary_c_pred, continuous_c_pred = model(x)\n\n            binary_c_predictions.append(binary_c_pred.detach().cpu().numpy())\n            continuous_c_predictions.append(continuous_c_pred.detach().cpu().numpy())\n            label_predictions.append(y_pred.detach().cpu().numpy())\n\n            c_loss = concept_loss(binary_c_pred, continuous_c_pred, c, binary_concept_idx)\n            y_loss = criterion(y_pred, y.unsqueeze(1).float())\n                \n            loss = y_loss + 0.5*c_loss \n\n            loss.backward()\n            optimizer.step()\n        \n        model.eval()\n\n        with torch.no_grad():\n            val_loss = 0.0\n            for x, c,_, y in val_loader:\n                \n                x, c, y = x.to(device), c.to(device), y.to(device)\n\n                ground_truth_val_c.append(c.cpu())\n                ground_truth_val_y.append(y.cpu())\n\n                y_pred, binary_c_pred, continuous_c_pred = model(x)\n\n                binary_c_val_predictions.append(binary_c_pred.detach().cpu().numpy())\n                continuous_c_val_predictions.append(continuous_c_pred.detach().cpu().numpy())\n                label_val_predictions.append(y_pred.detach().cpu().numpy())\n    \n                c_loss = concept_loss(binary_c_pred, continuous_c_pred, c, binary_concept_idx)\n                y_loss = criterion(y_pred, y.unsqueeze(1).float())\n                    \n                val_loss += y_loss + 0.5*c_loss \n\n        if stopper.should_stop(val_loss,model):\n            print(\"Done\")\n            # break\n        #scheduler.step()\n\n    return model, binary_c_predictions, continuous_c_predictions, label_predictions, binary_c_val_predictions, continuous_c_val_predictions, label_val_predictions, ground_truth_val_c, ground_truth_val_y","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T06:13:45.399423Z","iopub.execute_input":"2025-05-29T06:13:45.399796Z","iopub.status.idle":"2025-05-29T06:13:45.435595Z","shell.execute_reply.started":"2025-05-29T06:13:45.399767Z","shell.execute_reply":"2025-05-29T06:13:45.434414Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def evaluate_concept_predictor(ground_truth_c, binary_predictions, continuous_predictions, concept_labels, binary_concept_idx):\n    results = []\n\n    total_concepts = len(concept_labels)\n    continuous_idx = [i for i in range(total_concepts) if i not in binary_concept_idx]\n\n    for i, label in enumerate(concept_labels):\n\n        true_values = np.concatenate([c[:, i] if isinstance(c, np.ndarray) else c[:, i].numpy() for c in ground_truth_c])\n        \n        if i in binary_concept_idx:\n\n            predicted_values = np.concatenate([c[:, i - 12] for c in binary_predictions])\n            \n            predicted_classes = (predicted_values > 0.5).astype(int)\n\n            precision = round(precision_score(true_values, predicted_classes, zero_division=0), 3)\n            recall = round(recall_score(true_values, predicted_classes, zero_division=0), 3)\n            f1 = round(f1_score(true_values, predicted_classes, zero_division=0), 3)\n            accuracy = round(accuracy_score(true_values, predicted_classes), 3)\n\n            results.append({\n                \"Label\": label,\n                \"Precision\": precision,\n                \"Recall\": recall,\n                \"F1 Score\": f1,\n                \"Accuracy\": accuracy,\n            })\n\n        else:\n\n            predicted_values = np.concatenate([c[:, i] for c in continuous_predictions])\n            \n            mse = round(mean_squared_error(true_values, predicted_values), 3)\n            mae = round(mean_absolute_error(true_values, predicted_values), 3)\n            rmse = round(mean_squared_error(true_values, predicted_values, squared=False), 3)\n            r2 = round(r2_score(true_values, predicted_values), 3)\n\n            results.append({\n                \"Label\": label,\n                \"MSE\": mse,\n                \"MAE\": mae,\n                \"RMSE\": rmse,\n                \"R2\": r2\n            })\n\n    for label, result in zip(concept_labels, results):\n        print(result)\n            \n    return results\n\n# Label Predictor Evaluation\ndef evaluate_label_predictor(ground_truth_y, predicted_y):\n    true_values = np.concatenate(ground_truth_y)\n    predicted_values = np.concatenate(predicted_y).squeeze()\n\n    predicted_classes = (predicted_values > 0.5).astype(int)\n\n    precision = precision_score(true_values, predicted_classes)\n    recall = recall_score(true_values, predicted_classes)\n    f1 = f1_score(true_values, predicted_classes)\n    auc = roc_auc_score(true_values, predicted_classes)\n    accuracy = accuracy_score(true_values, predicted_classes)\n\n    results = {\n        \"Precision\": precision,\n        \"Recall\": recall,\n        \"F1 Score\": f1,\n        \"AUC\": auc,\n        \"Accuracy\": accuracy\n    }\n    return pd.DataFrame(results, index=[\"Metrics\"])\n\ndef test_model(model, test_loader, binary_concept_idx):\n    model.eval()  \n    criterion = nn.BCELoss()  \n\n    ground_truth_test_c, ground_truth_test_y = [], []\n    binary_c_test_predictions, continuous_c_test_predictions, label_test_predictions = [], [], []\n\n    model.eval()\n            \n    with torch.no_grad():\n        for x, c,_, y in test_loader:\n            \n            x, c, y = x.to(device), c.to(device), y.to(device)\n\n            ground_truth_test_c.append(c.cpu())\n            ground_truth_test_y.append(y.cpu())\n\n            y_pred, binary_c_pred, continuous_c_pred = model(x)\n\n            binary_c_test_predictions.append(binary_c_pred.detach().cpu().numpy())\n            continuous_c_test_predictions.append(continuous_c_pred.detach().cpu().numpy())\n            label_test_predictions.append(y_pred.detach().cpu().numpy())\n\n            c_loss = concept_loss(binary_c_pred, continuous_c_pred, c, binary_concept_idx)\n            y_loss = criterion(y_pred, y.unsqueeze(1).float())\n                \n            loss = y_loss + 0.5*c_loss \n            \n    return ground_truth_test_c, ground_truth_test_y, binary_c_test_predictions, continuous_c_test_predictions, label_test_predictions\n\ndef test_combined_model(model, test_loader, binary_concept_idx):\n    model.eval()\n    criterion = nn.BCELoss()\n\n    ground_truth_test_c, ground_truth_test_y = [], []\n    binary_c_test_predictions, continuous_c_test_predictions, label_test_predictions = [], [], []\n\n    with torch.no_grad():\n        for x, c, llm_c, y in test_loader:\n            x, c, llm_c, y = x.to(device), c.to(device), llm_c.to(device), y.to(device)\n\n            ground_truth_test_c.append(c.cpu())\n            ground_truth_test_y.append(y.cpu())\n            \n            y_pred, binary_c_pred, continuous_c_pred = model(x, llm_c)\n\n            binary_c_test_predictions.append(binary_c_pred.detach().cpu().numpy())\n            continuous_c_test_predictions.append(continuous_c_pred.detach().cpu().numpy())\n            label_test_predictions.append(y_pred.detach().cpu().numpy())\n\n            c_loss = concept_loss(binary_c_pred, continuous_c_pred, c, binary_concept_idx)\n            y_loss = criterion(y_pred, y.unsqueeze(1).float())\n                \n            loss = y_loss + 0.5*c_loss\n    \n    return ground_truth_test_c, ground_truth_test_y, binary_c_test_predictions, continuous_c_test_predictions, label_test_predictions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T06:13:45.439202Z","iopub.execute_input":"2025-05-29T06:13:45.439546Z","iopub.status.idle":"2025-05-29T06:13:45.464871Z","shell.execute_reply.started":"2025-05-29T06:13:45.439522Z","shell.execute_reply":"2025-05-29T06:13:45.464000Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Load Data\n# Check if GPU is available\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nfile_path = '/kaggle/input/mimic-ards/mlhc-ards-cohort-data.csv'\nprocessor = MIMICDataProcessor(file_path, batch_size=64)\ntrain_loader, val_loader, test_loader, hospital_test = processor.create_dataloaders()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T06:13:45.465792Z","iopub.execute_input":"2025-05-29T06:13:45.466062Z","iopub.status.idle":"2025-05-29T06:13:45.600553Z","shell.execute_reply.started":"2025-05-29T06:13:45.466042Z","shell.execute_reply":"2025-05-29T06:13:45.599519Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"x_size = processor.X_tensor_scaled_train.shape[1]\nc_size = processor.C_tensor_train.shape[1]\ny_size = 1\nx_to_y_learning_rate = 0.3\nweight_decay = 0.0001\nepochs = 100\nbinary_concept_idx = list(range(processor.C_cont.shape[1], processor.C_cont.shape[1] + processor.C_bin.shape[1]))\n\ntorch.manual_seed(25)\nmodel = MultiLabelNN1(21,2,12,1).to(device)\nmodel, binary_c_predictions, continuous_c_predictions, label_predictions, binary_c_val_predictions, continuous_c_val_predictions, label_val_predictions, ground_truth_val_c, ground_truth_val_y = train(model, x_size, c_size, y_size, x_to_y_learning_rate, weight_decay, epochs, train_loader, val_loader, binary_concept_idx)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T06:13:45.601766Z","iopub.execute_input":"2025-05-29T06:13:45.602105Z","iopub.status.idle":"2025-05-29T06:13:55.542860Z","shell.execute_reply.started":"2025-05-29T06:13:45.602076Z","shell.execute_reply":"2025-05-29T06:13:55.541878Z"}},"outputs":[{"name":"stdout","text":"Done\nDone\nDone\nDone\nDone\nDone\nDone\nDone\nDone\nDone\nDone\nDone\nDone\nDone\nDone\nDone\nDone\nDone\nDone\nDone\nDone\nDone\nDone\nDone\nDone\nDone\nDone\nDone\nDone\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"x_size = processor.X_tensor_scaled_train.shape[1]\nvanilla_c_size = processor.C_tensor_train.shape[1]\nllm_c_size = processor.LLM_C_tensor_train.shape[1]\n\ny_size = 1\nlearning_rate = 0.3\nepochs = 100\nweight_decay = 0.0001\n\ntorch.manual_seed(25)\nmodel2 = MultiLabelNN2(21,2,12,8,1).to(device)\n\nmodel2, binary_c_predictions_llm, continuous_c_predictions_llm, label_predictions_llm, binary_c_val_predictions_llm, continuous_c_val_predictions_llm, label_val_predictions_llm, ground_truth_val_c_llm, ground_truth_val_y_llm = train_combined_model(model2, x_size, vanilla_c_size, llm_c_size, y_size, learning_rate, epochs, train_loader, val_loader, binary_concept_idx, weight_decay)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T06:13:55.543918Z","iopub.execute_input":"2025-05-29T06:13:55.544592Z","iopub.status.idle":"2025-05-29T06:14:02.368602Z","shell.execute_reply.started":"2025-05-29T06:13:55.544559Z","shell.execute_reply":"2025-05-29T06:14:02.367332Z"}},"outputs":[{"name":"stdout","text":"Done\nDone\nDone\nDone\nDone\nDone\nDone\nDone\nDone\nDone\nDone\nDone\nDone\nDone\nDone\nDone\nDone\nDone\nDone\nDone\nDone\nDone\nDone\nDone\nDone\nDone\nDone\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"torch.save(model.state_dict(),\"cbm.pt\")\ntorch.save(model2.state_dict(),\"llm_cbm.pt\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T06:14:02.369762Z","iopub.execute_input":"2025-05-29T06:14:02.370040Z","iopub.status.idle":"2025-05-29T06:14:02.378085Z","shell.execute_reply.started":"2025-05-29T06:14:02.370019Z","shell.execute_reply":"2025-05-29T06:14:02.376851Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"c_true, y_true, binary_c_test_predictions, continuous_c_test_predictions, y_pred = test_model(model, test_loader, binary_concept_idx)\n\nc_true_llm, y_true_llm, binary_c_test_predictions_llm, continuous_c_test_predictions_llm, y_pred_llm = test_combined_model(model2, test_loader, binary_concept_idx)\n\nconcept_labels = processor.get_vanilla_concepts()\n\nprint(\"Vanilla\")\nvanilla_c_results = evaluate_concept_predictor(c_true,binary_c_test_predictions, continuous_c_test_predictions,concept_labels, binary_concept_idx)\nprint(\"LLM\")\nllm_c_results = evaluate_concept_predictor(c_true_llm,binary_c_test_predictions_llm, continuous_c_test_predictions_llm, concept_labels, binary_concept_idx)\n\nprint(evaluate_label_predictor(y_true, y_pred))\nprint(evaluate_label_predictor(y_true_llm, y_pred_llm))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T06:14:02.379331Z","iopub.execute_input":"2025-05-29T06:14:02.380366Z","iopub.status.idle":"2025-05-29T06:14:02.497800Z","shell.execute_reply.started":"2025-05-29T06:14:02.380337Z","shell.execute_reply":"2025-05-29T06:14:02.496799Z"}},"outputs":[{"name":"stdout","text":"Vanilla\n{'Label': 'c_sofa_avg_cardiovascular', 'MSE': 0.271, 'MAE': 0.19, 'RMSE': 0.521, 'R2': -3.288}\n{'Label': 'c_sofa_avg_respiration', 'MSE': 0.129, 'MAE': 0.271, 'RMSE': 0.359, 'R2': -3.911}\n{'Label': 'c_sofa_avg_renal', 'MSE': 1.864, 'MAE': 0.745, 'RMSE': 1.365, 'R2': -25.232}\n{'Label': 'c_sofa_avg_cns', 'MSE': 0.02, 'MAE': 0.104, 'RMSE': 0.14, 'R2': -3.042}\n{'Label': 'c_first24hr_sofa_max_cardiovascular', 'MSE': 0.421, 'MAE': 0.336, 'RMSE': 0.649, 'R2': -2.045}\n{'Label': 'c_first24hr_sofa_max_respiration', 'MSE': 0.16, 'MAE': 0.324, 'RMSE': 0.4, 'R2': -0.61}\n{'Label': 'c_first24hr_sofa_max_renal', 'MSE': 0.112, 'MAE': 0.275, 'RMSE': 0.335, 'R2': -0.031}\n{'Label': 'c_first24hr_sofa_max_cns', 'MSE': 0.171, 'MAE': 0.242, 'RMSE': 0.413, 'R2': -0.763}\n{'Label': 'c_sofa_max_cardiovascular', 'MSE': 0.653, 'MAE': 0.282, 'RMSE': 0.808, 'R2': -4.729}\n{'Label': 'c_sofa_max_respiration', 'MSE': 0.043, 'MAE': 0.151, 'RMSE': 0.206, 'R2': 0.511}\n{'Label': 'c_sofa_max_renal', 'MSE': 0.26, 'MAE': 0.405, 'RMSE': 0.51, 'R2': -0.696}\n{'Label': 'c_sofa_max_cns', 'MSE': 0.026, 'MAE': 0.129, 'RMSE': 0.161, 'R2': 0.791}\n{'Label': 'c_svr_resp_comorbidity', 'Precision': 0.969, 'Recall': 0.95, 'F1 Score': 0.96, 'Accuracy': 0.98}\n{'Label': 'c_mod_resp_comorbidity', 'Precision': 0.771, 'Recall': 0.955, 'F1 Score': 0.853, 'Accuracy': 0.777}\nLLM\n{'Label': 'c_sofa_avg_cardiovascular', 'MSE': 0.261, 'MAE': 0.174, 'RMSE': 0.511, 'R2': -3.129}\n{'Label': 'c_sofa_avg_respiration', 'MSE': 0.009, 'MAE': 0.073, 'RMSE': 0.094, 'R2': 0.665}\n{'Label': 'c_sofa_avg_renal', 'MSE': 0.052, 'MAE': 0.159, 'RMSE': 0.228, 'R2': 0.268}\n{'Label': 'c_sofa_avg_cns', 'MSE': 0.005, 'MAE': 0.051, 'RMSE': 0.067, 'R2': 0.069}\n{'Label': 'c_first24hr_sofa_max_cardiovascular', 'MSE': 0.434, 'MAE': 0.347, 'RMSE': 0.659, 'R2': -2.145}\n{'Label': 'c_first24hr_sofa_max_respiration', 'MSE': 0.11, 'MAE': 0.269, 'RMSE': 0.332, 'R2': -0.111}\n{'Label': 'c_first24hr_sofa_max_renal', 'MSE': 0.648, 'MAE': 0.393, 'RMSE': 0.805, 'R2': -4.941}\n{'Label': 'c_first24hr_sofa_max_cns', 'MSE': 0.096, 'MAE': 0.256, 'RMSE': 0.31, 'R2': 0.007}\n{'Label': 'c_sofa_max_cardiovascular', 'MSE': 0.55, 'MAE': 0.345, 'RMSE': 0.742, 'R2': -3.827}\n{'Label': 'c_sofa_max_respiration', 'MSE': 0.068, 'MAE': 0.141, 'RMSE': 0.261, 'R2': 0.216}\n{'Label': 'c_sofa_max_renal', 'MSE': 0.171, 'MAE': 0.215, 'RMSE': 0.414, 'R2': -0.117}\n{'Label': 'c_sofa_max_cns', 'MSE': 0.038, 'MAE': 0.139, 'RMSE': 0.195, 'R2': 0.693}\n{'Label': 'c_svr_resp_comorbidity', 'Precision': 0.97, 'Recall': 0.96, 'F1 Score': 0.965, 'Accuracy': 0.982}\n{'Label': 'c_mod_resp_comorbidity', 'Precision': 0.894, 'Recall': 0.955, 'F1 Score': 0.923, 'Accuracy': 0.893}\n         Precision    Recall  F1 Score       AUC  Accuracy\nMetrics   0.740113  0.629808  0.680519  0.689221  0.685422\n         Precision    Recall  F1 Score       AUC  Accuracy\nMetrics   0.771784  0.894231  0.828508  0.796842  0.803069\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from copy import deepcopy\nimport torch\nimport torch.nn as nn\n\nclass EarlyStopper:\n    def __init__(self, patience=25, verbose=False, delta=0, mode='min'):\n        self.patience = patience\n        self.verbose = verbose\n        self.counter = 0\n        self.best_score = None\n        self.early_stop = False\n        self.val_loss_min = float('inf') # For 'min' mode\n        self.delta = delta # Minimum change to qualify as an improvement\n        self.best_model_state = None\n        self.mode = mode # 'min' for loss, 'max' for accuracy/AUC\n\n    def should_stop(self, score, model):\n        if self.best_score is None:\n            self.best_score = score\n            self.save_checkpoint(score, model)\n        elif (self.mode == 'min' and score < self.best_score - self.delta) or \\\n             (self.mode == 'max' and score > self.best_score + self.delta):\n            self.best_score = score\n            self.save_checkpoint(score, model)\n            self.counter = 0\n        else:\n            self.counter += 1\n            if self.verbose:\n                print(f'EarlyStopper counter: {self.counter} out of {self.patience}')\n            if self.counter >= self.patience:\n                self.early_stop = True\n        \n        return self.early_stop\n\n    def save_checkpoint(self, score, model):\n        '''Saves model when validation score improves.'''\n        if self.verbose:\n            if self.mode == 'min':\n                print(f'Validation score decreased ({self.val_loss_min:.6f} --> {score:.6f}).  Saving model ...')\n            else:\n                print(f'Validation score increased ({self.val_loss_min:.6f} --> {score:.6f}).  Saving model ...')\n        self.best_model_state = deepcopy(model.state_dict())\n        self.val_loss_min = score # Keep track of the actual best score\n\n    def load_best_model(self, model):\n        if self.best_model_state is not None:\n            model.load_state_dict(self.best_model_state)\n            if self.verbose:\n                print(\"Loaded best model state from early stopping.\")\n        return model\n\n# Define your existing criterion and concept_loss functions (from your notebook)\ncriterion = nn.BCELoss() # For final label prediction\n\ndef concept_loss(binary_c_pred, continuous_c_pred, vanilla_c, binary_concept_idx):\n    bce_loss = nn.BCELoss()\n    mse_loss = nn.MSELoss()\n    \n    num_total_concepts = vanilla_c.shape[1]\n    \n    # Assuming binary_concept_idx contains the indices of binary concepts in the `vanilla_c` tensor\n    # And that binary_c_pred corresponds to these.\n    binary_loss_val = 0\n    if binary_concept_idx and binary_c_pred.numel() > 0 : # Check if there are binary concepts to predict\n        true_binary_concepts = vanilla_c[:, binary_concept_idx]\n        binary_loss_val = bce_loss(binary_c_pred, true_binary_concepts)\n\n    # Continuous concepts are those not in binary_concept_idx\n    continuous_concept_indices = [i for i in range(num_total_concepts) if i not in binary_concept_idx]\n    continuous_loss_val = 0\n    if continuous_concept_indices and continuous_c_pred.numel() > 0: # Check if there are continuous concepts\n        true_continuous_concepts = vanilla_c[:, continuous_concept_indices]\n        continuous_loss_val = mse_loss(continuous_c_pred, true_continuous_concepts)\n        \n    return binary_loss_val + continuous_loss_val\n\ndef train_x_to_c_sequential(model, device, train_loader, val_loader, binary_concept_idx, \n                            learning_rate, weight_decay, epochs, patience=10):\n    print(\"--- Stage 1: Training x -> c (Concepts) ---\")\n    \n    # Set requires_grad for layers\n    for param in model.layer1.parameters(): param.requires_grad = True\n    for param in model.layer2.parameters(): param.requires_grad = True\n    for param in model.layer3.parameters(): param.requires_grad = False\n    for param in model.layer4.parameters(): param.requires_grad = False\n\n    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), \n                                 lr=learning_rate, weight_decay=weight_decay)\n    \n    stopper = EarlyStopper(patience=patience, verbose=True, mode='min')\n\n    for epoch in range(epochs):\n        model.train()\n        train_loss_sum = 0\n        for x_batch, c_true_batch, _, _ in train_loader: # Use c_true_batch for concept truth\n            x_batch, c_true_batch = x_batch.to(device), c_true_batch.to(device)\n            \n            optimizer.zero_grad()\n            _, binary_c_pred, continuous_c_pred = model(x_batch)\n            \n            loss = concept_loss(binary_c_pred, continuous_c_pred, c_true_batch, binary_concept_idx)\n            loss.backward()\n            optimizer.step()\n            train_loss_sum += loss.item()\n\n        model.eval()\n        val_loss_sum = 0\n        with torch.no_grad():\n            for x_batch, c_true_batch, _, _ in val_loader:\n                x_batch, c_true_batch = x_batch.to(device), c_true_batch.to(device)\n                _, binary_c_pred, continuous_c_pred = model(x_batch)\n                loss = concept_loss(binary_c_pred, continuous_c_pred, c_true_batch, binary_concept_idx)\n                val_loss_sum += loss.item()\n        \n        avg_train_loss = train_loss_sum / len(train_loader)\n        avg_val_loss = val_loss_sum / len(val_loader)\n        print(f\"Epoch {epoch+1}/{epochs} [x->c] - Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n\n        if stopper.should_stop(avg_val_loss, model):\n            print(f\"Early stopping at epoch {epoch+1} for x->c training.\")\n            break\n            \n    model = stopper.load_best_model(model) # Load the best model state for concept layers\n    return model\n\ndef train_c_to_y_sequential(model, device, train_loader, val_loader, \n                            learning_rate, weight_decay, epochs, patience=10):\n    print(\"--- Stage 2: Training c -> y (Labels from Predicted Concepts) ---\")\n\n    # Set requires_grad for layers\n    for param in model.layer1.parameters(): param.requires_grad = False # Freeze concept layers\n    for param in model.layer2.parameters(): param.requires_grad = False\n    for param in model.layer3.parameters(): param.requires_grad = True\n    for param in model.layer4.parameters(): param.requires_grad = True\n\n    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), \n                                 lr=learning_rate, weight_decay=weight_decay)\n    \n    stopper = EarlyStopper(patience=patience, verbose=True, mode='min') # Loss is minimized\n\n    for epoch in range(epochs):\n        model.train()\n        train_loss_sum = 0\n        for x_batch, _, _, y_true_batch in train_loader: # Need x to generate concepts\n            x_batch, y_true_batch = x_batch.to(device), y_true_batch.to(device)\n\n            optimizer.zero_grad()\n            \n            # Get predicted concepts from the frozen x->c part\n            # The model's forward pass gives y_pred, binary_c, continuous_c\n            # We need to ensure that the y_pred used for loss here is ONLY from the c->y part\n            with torch.no_grad(): # Ensure concept layers are not updated during this intermediate step\n                 _, binary_c_intermediate, continuous_c_intermediate = model(x_batch)\n            \n            # Now use these *detached* predicted concepts to predict y through the trainable layers\n            # Manually compute y_pred from the specific layers to ensure correct grad flow\n            y_pred = torch.sigmoid(model.layer3(binary_c_intermediate.detach()) + \\\n                                   model.layer4(continuous_c_intermediate.detach()))\n\n            loss = criterion(y_pred, y_true_batch.unsqueeze(1).float())\n            loss.backward()\n            optimizer.step()\n            train_loss_sum += loss.item()\n\n        model.eval()\n        val_loss_sum = 0\n        with torch.no_grad():\n            for x_batch, _, _, y_true_batch in val_loader:\n                x_batch, y_true_batch = x_batch.to(device), y_true_batch.to(device)\n                \n                _, binary_c_intermediate, continuous_c_intermediate = model(x_batch)\n                y_pred = torch.sigmoid(model.layer3(binary_c_intermediate) + \\\n                                       model.layer4(continuous_c_intermediate))\n                \n                loss = criterion(y_pred, y_true_batch.unsqueeze(1).float())\n                val_loss_sum += loss.item()\n\n        avg_train_loss = train_loss_sum / len(train_loader)\n        avg_val_loss = val_loss_sum / len(val_loader)\n        print(f\"Epoch {epoch+1}/{epochs} [c->y] - Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n        \n        if stopper.should_stop(avg_val_loss, model):\n            print(f\"Early stopping at epoch {epoch+1} for c->y training.\")\n            break\n            \n    model = stopper.load_best_model(model) # Load the best model state for label layers\n    return model\n\ndef train_x_to_c_sequential_combined(model, device, train_loader, val_loader, binary_concept_idx, \n                                     learning_rate, weight_decay, epochs, patience=10):\n    print(\"--- Stage 1 (Combined Model): Training x -> c (Concepts) ---\")\n    \n    # Set requires_grad for layers\n    for param in model.layer1.parameters(): param.requires_grad = True\n    for param in model.layer2.parameters(): param.requires_grad = True\n    for param in model.layer3.parameters(): param.requires_grad = False\n    for param in model.layer4.parameters(): param.requires_grad = False\n    for param in model.layer5.parameters(): param.requires_grad = False # LLM concept layer also frozen\n\n    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), \n                                 lr=learning_rate, weight_decay=weight_decay)\n    \n    stopper = EarlyStopper(patience=patience, verbose=True, mode='min')\n\n    for epoch in range(epochs):\n        model.train()\n        train_loss_sum = 0\n        for x_batch, c_true_batch, llm_c_batch, _ in train_loader:\n            x_batch, c_true_batch, llm_c_batch = x_batch.to(device), c_true_batch.to(device), llm_c_batch.to(device)\n            \n            optimizer.zero_grad()\n            # model's forward for MultiLabelNN2 needs x and llm_c\n            _, binary_c_pred, continuous_c_pred = model(x_batch, llm_c_batch) \n            \n            loss = concept_loss(binary_c_pred, continuous_c_pred, c_true_batch, binary_concept_idx)\n            loss.backward()\n            optimizer.step()\n            train_loss_sum += loss.item()\n\n        model.eval()\n        val_loss_sum = 0\n        with torch.no_grad():\n            for x_batch, c_true_batch, llm_c_batch, _ in val_loader:\n                x_batch, c_true_batch, llm_c_batch = x_batch.to(device), c_true_batch.to(device), llm_c_batch.to(device)\n                _, binary_c_pred, continuous_c_pred = model(x_batch, llm_c_batch)\n                loss = concept_loss(binary_c_pred, continuous_c_pred, c_true_batch, binary_concept_idx)\n                val_loss_sum += loss.item()\n        \n        avg_train_loss = train_loss_sum / len(train_loader)\n        avg_val_loss = val_loss_sum / len(val_loader)\n        print(f\"Epoch {epoch+1}/{epochs} [x->c Combined] - Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n\n        if stopper.should_stop(avg_val_loss, model):\n            print(f\"Early stopping at epoch {epoch+1} for x->c combined training.\")\n            break\n            \n    model = stopper.load_best_model(model)\n    return model\n\ndef train_c_to_y_sequential_combined(model, device, train_loader, val_loader, \n                                     learning_rate, weight_decay, epochs, patience=10):\n    print(\"--- Stage 2 (Combined Model): Training c -> y (Labels from Predicted & LLM Concepts) ---\")\n\n    # Set requires_grad for layers\n    for param in model.layer1.parameters(): param.requires_grad = False # Freeze concept layers\n    for param in model.layer2.parameters(): param.requires_grad = False\n    for param in model.layer3.parameters(): param.requires_grad = True\n    for param in model.layer4.parameters(): param.requires_grad = True\n    for param in model.layer5.parameters(): param.requires_grad = True # LLM concept layer is trained\n\n    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), \n                                 lr=learning_rate, weight_decay=weight_decay)\n        \n    stopper = EarlyStopper(patience=patience, verbose=True, mode='min')\n\n    for epoch in range(epochs):\n        model.train()\n        train_loss_sum = 0\n        for x_batch, _, llm_c_batch, y_true_batch in train_loader:\n            x_batch, llm_c_batch, y_true_batch = x_batch.to(device), llm_c_batch.to(device), y_true_batch.to(device)\n\n            optimizer.zero_grad()\n            \n            with torch.no_grad():\n                 _, binary_c_intermediate, continuous_c_intermediate = model(x_batch, llm_c_batch)\n            \n            # Manually compute y_pred from the specific layers\n            y_pred = torch.sigmoid(model.layer3(binary_c_intermediate.detach()) + \\\n                                   model.layer4(continuous_c_intermediate.detach()) + \\\n                                   model.layer5(llm_c_batch)) # llm_c_batch is a direct input here\n\n            loss = criterion(y_pred, y_true_batch.unsqueeze(1).float())\n            loss.backward()\n            optimizer.step()\n            train_loss_sum += loss.item()\n\n        model.eval()\n        val_loss_sum = 0\n        with torch.no_grad():\n            for x_batch, _, llm_c_batch, y_true_batch in val_loader:\n                x_batch, llm_c_batch, y_true_batch = x_batch.to(device), llm_c_batch.to(device), y_true_batch.to(device)\n                \n                _, binary_c_intermediate, continuous_c_intermediate = model(x_batch, llm_c_batch)\n                y_pred = torch.sigmoid(model.layer3(binary_c_intermediate) + \\\n                                       model.layer4(continuous_c_intermediate) + \\\n                                       model.layer5(llm_c_batch))\n                \n                loss = criterion(y_pred, y_true_batch.unsqueeze(1).float())\n                val_loss_sum += loss.item()\n\n        avg_train_loss = train_loss_sum / len(train_loader)\n        avg_val_loss = val_loss_sum / len(val_loader)\n        print(f\"Epoch {epoch+1}/{epochs} [c->y Combined] - Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n        \n        if stopper.should_stop(avg_val_loss, model):\n            print(f\"Early stopping at epoch {epoch+1} for c->y combined training.\")\n            break\n            \n    model = stopper.load_best_model(model)\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T06:14:02.499023Z","iopub.execute_input":"2025-05-29T06:14:02.499341Z","iopub.status.idle":"2025-05-29T06:14:02.539231Z","shell.execute_reply.started":"2025-05-29T06:14:02.499306Z","shell.execute_reply":"2025-05-29T06:14:02.538150Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# Assuming 'processor', 'train_loader', 'val_loader', 'binary_concept_idx', 'device' are defined\n# And MultiLabelNN1, MultiLabelNN2 are defined.\n\n# --- Training MultiLabelNN1 Sequentially ---\nx_size = processor.X_tensor_scaled_train.shape[1]\n# c_size = processor.C_tensor_train.shape[1] # Not directly used in sequential setup in this manner\n# y_size = 1\n# For MultiLabelNN1, C_cont is 12, C_bin is 2.\nnum_continuous_concepts_model1 = processor.C_cont.shape[1]\nnum_binary_concepts_model1 = processor.C_bin.shape[1]\n\ntorch.manual_seed(25)\nmodel1_sequential = MultiLabelNN1(num_features=x_size, \n                                  num_binary_concepts=num_binary_concepts_model1, \n                                  num_continuous_concepts=num_continuous_concepts_model1, \n                                  num_labels=1).to(device)\n\n# Stage 1 for model1\nlr_stage1_m1 = 0.1 # Example\nwd_stage1_m1 = 0.001\nepochs_stage1_m1 = 50 # Example\npatience_stage1_m1 = 15\nmodel1_sequential = train_x_to_c_sequential(model1_sequential, device, train_loader, val_loader, \n                                            binary_concept_idx, lr_stage1_m1, wd_stage1_m1, \n                                            epochs_stage1_m1, patience_stage1_m1)\n\n# Stage 2 for model1\nlr_stage2_m1 = 0.1 # Example\nwd_stage2_m1 = 0.001\nepochs_stage2_m1 = 50 # Example\npatience_stage2_m1 = 15\nmodel1_sequential = train_c_to_y_sequential(model1_sequential, device, train_loader, val_loader, \n                                            lr_stage2_m1, wd_stage2_m1, epochs_stage2_m1, patience_stage2_m1)\n\nprint(\"MultiLabelNN1 (Standard CBM) trained sequentially.\")\n# You can now evaluate model1_sequential using your test_model function\n\n\n# --- Training MultiLabelNN2 Sequentially ---\nllm_c_size = processor.LLM_C_tensor_train.shape[1] # Should be 8\nnum_continuous_concepts_model2 = processor.C_cont.shape[1] # 12\nnum_binary_concepts_model2 = processor.C_bin.shape[1] # 2\n\n\ntorch.manual_seed(25)\nmodel2_sequential = MultiLabelNN2(num_features=x_size, \n                                  num_binary_concepts=num_binary_concepts_model2, \n                                  num_continuous_concepts=num_continuous_concepts_model2, \n                                  num_llm_concepts=llm_c_size, \n                                  num_labels=1).to(device)\n\n# Stage 1 for model2\nlr_stage1_m2 = 0.1\nwd_stage1_m2 = 0.001\nepochs_stage1_m2 = 50\npatience_stage1_m2 = 15\nmodel2_sequential = train_x_to_c_sequential_combined(model2_sequential, device, train_loader, val_loader, \n                                                     binary_concept_idx, lr_stage1_m2, wd_stage1_m2, \n                                                     epochs_stage1_m2, patience_stage1_m2)\n\n# Stage 2 for model2\nlr_stage2_m2 = 0.1\nwd_stage2_m2 = 0.001\nepochs_stage2_m2 = 50\npatience_stage2_m2 = 15\nmodel2_sequential = train_c_to_y_sequential_combined(model2_sequential, device, train_loader, val_loader, \n                                                     lr_stage2_m2, wd_stage2_m2, epochs_stage2_m2, patience_stage2_m2)\n\nprint(\"MultiLabelNN2 (Context-Aware CBM) trained sequentially.\")\n# You can now evaluate model2_sequential using your test_combined_model function\ntorch.save(model1_sequential.state_dict(), \"cbm_sequential.pt\")\ntorch.save(model2_sequential.state_dict(), \"llm_cbm_sequential.pt\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T06:14:02.540914Z","iopub.execute_input":"2025-05-29T06:14:02.541229Z","iopub.status.idle":"2025-05-29T06:14:10.984140Z","shell.execute_reply.started":"2025-05-29T06:14:02.541195Z","shell.execute_reply":"2025-05-29T06:14:10.982924Z"}},"outputs":[{"name":"stdout","text":"--- Stage 1: Training x -> c (Concepts) ---\nEpoch 1/50 [x->c] - Train Loss: 0.6065, Val Loss: 0.4235\nValidation score decreased (inf --> 0.423480).  Saving model ...\nEpoch 2/50 [x->c] - Train Loss: 0.3593, Val Loss: 0.3135\nValidation score decreased (0.423480 --> 0.313530).  Saving model ...\nEpoch 3/50 [x->c] - Train Loss: 0.2949, Val Loss: 0.2807\nValidation score decreased (0.313530 --> 0.280692).  Saving model ...\nEpoch 4/50 [x->c] - Train Loss: 0.2722, Val Loss: 0.2715\nValidation score decreased (0.280692 --> 0.271479).  Saving model ...\nEpoch 5/50 [x->c] - Train Loss: 0.2602, Val Loss: 0.2561\nValidation score decreased (0.271479 --> 0.256062).  Saving model ...\nEpoch 6/50 [x->c] - Train Loss: 0.2544, Val Loss: 0.2557\nValidation score decreased (0.256062 --> 0.255698).  Saving model ...\nEpoch 7/50 [x->c] - Train Loss: 0.2534, Val Loss: 0.2500\nValidation score decreased (0.255698 --> 0.249955).  Saving model ...\nEpoch 8/50 [x->c] - Train Loss: 0.2486, Val Loss: 0.2473\nValidation score decreased (0.249955 --> 0.247255).  Saving model ...\nEpoch 9/50 [x->c] - Train Loss: 0.2430, Val Loss: 0.2624\nEarlyStopper counter: 1 out of 15\nEpoch 10/50 [x->c] - Train Loss: 0.2454, Val Loss: 0.2477\nEarlyStopper counter: 2 out of 15\nEpoch 11/50 [x->c] - Train Loss: 0.2463, Val Loss: 0.2466\nValidation score decreased (0.247255 --> 0.246592).  Saving model ...\nEpoch 12/50 [x->c] - Train Loss: 0.2401, Val Loss: 0.2436\nValidation score decreased (0.246592 --> 0.243576).  Saving model ...\nEpoch 13/50 [x->c] - Train Loss: 0.2491, Val Loss: 0.2434\nValidation score decreased (0.243576 --> 0.243394).  Saving model ...\nEpoch 14/50 [x->c] - Train Loss: 0.2429, Val Loss: 0.2417\nValidation score decreased (0.243394 --> 0.241685).  Saving model ...\nEpoch 15/50 [x->c] - Train Loss: 0.2387, Val Loss: 0.2515\nEarlyStopper counter: 1 out of 15\nEpoch 16/50 [x->c] - Train Loss: 0.2461, Val Loss: 0.2576\nEarlyStopper counter: 2 out of 15\nEpoch 17/50 [x->c] - Train Loss: 0.2492, Val Loss: 0.2519\nEarlyStopper counter: 3 out of 15\nEpoch 18/50 [x->c] - Train Loss: 0.2478, Val Loss: 0.2527\nEarlyStopper counter: 4 out of 15\nEpoch 19/50 [x->c] - Train Loss: 0.2413, Val Loss: 0.2494\nEarlyStopper counter: 5 out of 15\nEpoch 20/50 [x->c] - Train Loss: 0.2402, Val Loss: 0.2419\nEarlyStopper counter: 6 out of 15\nEpoch 21/50 [x->c] - Train Loss: 0.2436, Val Loss: 0.2433\nEarlyStopper counter: 7 out of 15\nEpoch 22/50 [x->c] - Train Loss: 0.2488, Val Loss: 0.2421\nEarlyStopper counter: 8 out of 15\nEpoch 23/50 [x->c] - Train Loss: 0.2411, Val Loss: 0.2458\nEarlyStopper counter: 9 out of 15\nEpoch 24/50 [x->c] - Train Loss: 0.2410, Val Loss: 0.2503\nEarlyStopper counter: 10 out of 15\nEpoch 25/50 [x->c] - Train Loss: 0.2511, Val Loss: 0.2580\nEarlyStopper counter: 11 out of 15\nEpoch 26/50 [x->c] - Train Loss: 0.2468, Val Loss: 0.2540\nEarlyStopper counter: 12 out of 15\nEpoch 27/50 [x->c] - Train Loss: 0.2489, Val Loss: 0.2616\nEarlyStopper counter: 13 out of 15\nEpoch 28/50 [x->c] - Train Loss: 0.2442, Val Loss: 0.2471\nEarlyStopper counter: 14 out of 15\nEpoch 29/50 [x->c] - Train Loss: 0.2401, Val Loss: 0.2419\nEarlyStopper counter: 15 out of 15\nEarly stopping at epoch 29 for x->c training.\nLoaded best model state from early stopping.\n--- Stage 2: Training c -> y (Labels from Predicted Concepts) ---\nEpoch 1/50 [c->y] - Train Loss: 0.7067, Val Loss: 0.6593\nValidation score decreased (inf --> 0.659262).  Saving model ...\nEpoch 2/50 [c->y] - Train Loss: 0.6518, Val Loss: 0.6522\nValidation score decreased (0.659262 --> 0.652200).  Saving model ...\nEpoch 3/50 [c->y] - Train Loss: 0.6430, Val Loss: 0.6538\nEarlyStopper counter: 1 out of 15\nEpoch 4/50 [c->y] - Train Loss: 0.6481, Val Loss: 0.6433\nValidation score decreased (0.652200 --> 0.643339).  Saving model ...\nEpoch 5/50 [c->y] - Train Loss: 0.6367, Val Loss: 0.6374\nValidation score decreased (0.643339 --> 0.637430).  Saving model ...\nEpoch 6/50 [c->y] - Train Loss: 0.6392, Val Loss: 0.6346\nValidation score decreased (0.637430 --> 0.634583).  Saving model ...\nEpoch 7/50 [c->y] - Train Loss: 0.6422, Val Loss: 0.6440\nEarlyStopper counter: 1 out of 15\nEpoch 8/50 [c->y] - Train Loss: 0.6292, Val Loss: 0.6330\nValidation score decreased (0.634583 --> 0.633027).  Saving model ...\nEpoch 9/50 [c->y] - Train Loss: 0.6366, Val Loss: 0.6383\nEarlyStopper counter: 1 out of 15\nEpoch 10/50 [c->y] - Train Loss: 0.6320, Val Loss: 0.6299\nValidation score decreased (0.633027 --> 0.629892).  Saving model ...\nEpoch 11/50 [c->y] - Train Loss: 0.6517, Val Loss: 0.6311\nEarlyStopper counter: 1 out of 15\nEpoch 12/50 [c->y] - Train Loss: 0.6279, Val Loss: 0.6248\nValidation score decreased (0.629892 --> 0.624779).  Saving model ...\nEpoch 13/50 [c->y] - Train Loss: 0.6298, Val Loss: 0.6656\nEarlyStopper counter: 1 out of 15\nEpoch 14/50 [c->y] - Train Loss: 0.6409, Val Loss: 0.6426\nEarlyStopper counter: 2 out of 15\nEpoch 15/50 [c->y] - Train Loss: 0.6211, Val Loss: 0.6230\nValidation score decreased (0.624779 --> 0.623039).  Saving model ...\nEpoch 16/50 [c->y] - Train Loss: 0.6242, Val Loss: 0.6229\nValidation score decreased (0.623039 --> 0.622933).  Saving model ...\nEpoch 17/50 [c->y] - Train Loss: 0.6327, Val Loss: 0.6228\nValidation score decreased (0.622933 --> 0.622762).  Saving model ...\nEpoch 18/50 [c->y] - Train Loss: 0.6330, Val Loss: 0.6385\nEarlyStopper counter: 1 out of 15\nEpoch 19/50 [c->y] - Train Loss: 0.6261, Val Loss: 0.6361\nEarlyStopper counter: 2 out of 15\nEpoch 20/50 [c->y] - Train Loss: 0.6224, Val Loss: 0.6199\nValidation score decreased (0.622762 --> 0.619863).  Saving model ...\nEpoch 21/50 [c->y] - Train Loss: 0.6304, Val Loss: 0.6479\nEarlyStopper counter: 1 out of 15\nEpoch 22/50 [c->y] - Train Loss: 0.6327, Val Loss: 0.6223\nEarlyStopper counter: 2 out of 15\nEpoch 23/50 [c->y] - Train Loss: 0.6264, Val Loss: 0.6198\nValidation score decreased (0.619863 --> 0.619754).  Saving model ...\nEpoch 24/50 [c->y] - Train Loss: 0.6212, Val Loss: 0.6307\nEarlyStopper counter: 1 out of 15\nEpoch 25/50 [c->y] - Train Loss: 0.6219, Val Loss: 0.6217\nEarlyStopper counter: 2 out of 15\nEpoch 26/50 [c->y] - Train Loss: 0.6328, Val Loss: 0.6330\nEarlyStopper counter: 3 out of 15\nEpoch 27/50 [c->y] - Train Loss: 0.6329, Val Loss: 0.6252\nEarlyStopper counter: 4 out of 15\nEpoch 28/50 [c->y] - Train Loss: 0.6272, Val Loss: 0.6242\nEarlyStopper counter: 5 out of 15\nEpoch 29/50 [c->y] - Train Loss: 0.6233, Val Loss: 0.6216\nEarlyStopper counter: 6 out of 15\nEpoch 30/50 [c->y] - Train Loss: 0.6187, Val Loss: 0.6292\nEarlyStopper counter: 7 out of 15\nEpoch 31/50 [c->y] - Train Loss: 0.6193, Val Loss: 0.6387\nEarlyStopper counter: 8 out of 15\nEpoch 32/50 [c->y] - Train Loss: 0.6304, Val Loss: 0.6173\nValidation score decreased (0.619754 --> 0.617340).  Saving model ...\nEpoch 33/50 [c->y] - Train Loss: 0.6230, Val Loss: 0.6185\nEarlyStopper counter: 1 out of 15\nEpoch 34/50 [c->y] - Train Loss: 0.6178, Val Loss: 0.6206\nEarlyStopper counter: 2 out of 15\nEpoch 35/50 [c->y] - Train Loss: 0.6142, Val Loss: 0.6195\nEarlyStopper counter: 3 out of 15\nEpoch 36/50 [c->y] - Train Loss: 0.6226, Val Loss: 0.6178\nEarlyStopper counter: 4 out of 15\nEpoch 37/50 [c->y] - Train Loss: 0.6292, Val Loss: 0.6195\nEarlyStopper counter: 5 out of 15\nEpoch 38/50 [c->y] - Train Loss: 0.6212, Val Loss: 0.6192\nEarlyStopper counter: 6 out of 15\nEpoch 39/50 [c->y] - Train Loss: 0.6217, Val Loss: 0.6178\nEarlyStopper counter: 7 out of 15\nEpoch 40/50 [c->y] - Train Loss: 0.6170, Val Loss: 0.6238\nEarlyStopper counter: 8 out of 15\nEpoch 41/50 [c->y] - Train Loss: 0.6166, Val Loss: 0.6193\nEarlyStopper counter: 9 out of 15\nEpoch 42/50 [c->y] - Train Loss: 0.6206, Val Loss: 0.6175\nEarlyStopper counter: 10 out of 15\nEpoch 43/50 [c->y] - Train Loss: 0.6197, Val Loss: 0.6331\nEarlyStopper counter: 11 out of 15\nEpoch 44/50 [c->y] - Train Loss: 0.6391, Val Loss: 0.6326\nEarlyStopper counter: 12 out of 15\nEpoch 45/50 [c->y] - Train Loss: 0.6243, Val Loss: 0.6205\nEarlyStopper counter: 13 out of 15\nEpoch 46/50 [c->y] - Train Loss: 0.6226, Val Loss: 0.6191\nEarlyStopper counter: 14 out of 15\nEpoch 47/50 [c->y] - Train Loss: 0.6264, Val Loss: 0.6199\nEarlyStopper counter: 15 out of 15\nEarly stopping at epoch 47 for c->y training.\nLoaded best model state from early stopping.\nMultiLabelNN1 (Standard CBM) trained sequentially.\n--- Stage 1 (Combined Model): Training x -> c (Concepts) ---\nEpoch 1/50 [x->c Combined] - Train Loss: 0.5933, Val Loss: 0.4174\nValidation score decreased (inf --> 0.417362).  Saving model ...\nEpoch 2/50 [x->c Combined] - Train Loss: 0.3468, Val Loss: 0.3082\nValidation score decreased (0.417362 --> 0.308239).  Saving model ...\nEpoch 3/50 [x->c Combined] - Train Loss: 0.2888, Val Loss: 0.2751\nValidation score decreased (0.308239 --> 0.275080).  Saving model ...\nEpoch 4/50 [x->c Combined] - Train Loss: 0.2659, Val Loss: 0.2683\nValidation score decreased (0.275080 --> 0.268329).  Saving model ...\nEpoch 5/50 [x->c Combined] - Train Loss: 0.2597, Val Loss: 0.2682\nValidation score decreased (0.268329 --> 0.268225).  Saving model ...\nEpoch 6/50 [x->c Combined] - Train Loss: 0.2551, Val Loss: 0.2604\nValidation score decreased (0.268225 --> 0.260439).  Saving model ...\nEpoch 7/50 [x->c Combined] - Train Loss: 0.2461, Val Loss: 0.2571\nValidation score decreased (0.260439 --> 0.257061).  Saving model ...\nEpoch 8/50 [x->c Combined] - Train Loss: 0.2459, Val Loss: 0.2614\nEarlyStopper counter: 1 out of 15\nEpoch 9/50 [x->c Combined] - Train Loss: 0.2467, Val Loss: 0.2448\nValidation score decreased (0.257061 --> 0.244814).  Saving model ...\nEpoch 10/50 [x->c Combined] - Train Loss: 0.2445, Val Loss: 0.2478\nEarlyStopper counter: 1 out of 15\nEpoch 11/50 [x->c Combined] - Train Loss: 0.2446, Val Loss: 0.2436\nValidation score decreased (0.244814 --> 0.243554).  Saving model ...\nEpoch 12/50 [x->c Combined] - Train Loss: 0.2435, Val Loss: 0.2389\nValidation score decreased (0.243554 --> 0.238891).  Saving model ...\nEpoch 13/50 [x->c Combined] - Train Loss: 0.2454, Val Loss: 0.2433\nEarlyStopper counter: 1 out of 15\nEpoch 14/50 [x->c Combined] - Train Loss: 0.2431, Val Loss: 0.2486\nEarlyStopper counter: 2 out of 15\nEpoch 15/50 [x->c Combined] - Train Loss: 0.2429, Val Loss: 0.2484\nEarlyStopper counter: 3 out of 15\nEpoch 16/50 [x->c Combined] - Train Loss: 0.2478, Val Loss: 0.2574\nEarlyStopper counter: 4 out of 15\nEpoch 17/50 [x->c Combined] - Train Loss: 0.2437, Val Loss: 0.2514\nEarlyStopper counter: 5 out of 15\nEpoch 18/50 [x->c Combined] - Train Loss: 0.2397, Val Loss: 0.2407\nEarlyStopper counter: 6 out of 15\nEpoch 19/50 [x->c Combined] - Train Loss: 0.2433, Val Loss: 0.2463\nEarlyStopper counter: 7 out of 15\nEpoch 20/50 [x->c Combined] - Train Loss: 0.2408, Val Loss: 0.2409\nEarlyStopper counter: 8 out of 15\nEpoch 21/50 [x->c Combined] - Train Loss: 0.2501, Val Loss: 0.2434\nEarlyStopper counter: 9 out of 15\nEpoch 22/50 [x->c Combined] - Train Loss: 0.2491, Val Loss: 0.2376\nValidation score decreased (0.238891 --> 0.237648).  Saving model ...\nEpoch 23/50 [x->c Combined] - Train Loss: 0.2406, Val Loss: 0.2517\nEarlyStopper counter: 1 out of 15\nEpoch 24/50 [x->c Combined] - Train Loss: 0.2415, Val Loss: 0.2461\nEarlyStopper counter: 2 out of 15\nEpoch 25/50 [x->c Combined] - Train Loss: 0.2423, Val Loss: 0.2421\nEarlyStopper counter: 3 out of 15\nEpoch 26/50 [x->c Combined] - Train Loss: 0.2533, Val Loss: 0.2453\nEarlyStopper counter: 4 out of 15\nEpoch 27/50 [x->c Combined] - Train Loss: 0.2399, Val Loss: 0.2464\nEarlyStopper counter: 5 out of 15\nEpoch 28/50 [x->c Combined] - Train Loss: 0.2404, Val Loss: 0.2894\nEarlyStopper counter: 6 out of 15\nEpoch 29/50 [x->c Combined] - Train Loss: 0.2432, Val Loss: 0.2646\nEarlyStopper counter: 7 out of 15\nEpoch 30/50 [x->c Combined] - Train Loss: 0.2447, Val Loss: 0.2486\nEarlyStopper counter: 8 out of 15\nEpoch 31/50 [x->c Combined] - Train Loss: 0.2413, Val Loss: 0.2480\nEarlyStopper counter: 9 out of 15\nEpoch 32/50 [x->c Combined] - Train Loss: 0.2389, Val Loss: 0.2423\nEarlyStopper counter: 10 out of 15\nEpoch 33/50 [x->c Combined] - Train Loss: 0.2409, Val Loss: 0.2671\nEarlyStopper counter: 11 out of 15\nEpoch 34/50 [x->c Combined] - Train Loss: 0.2482, Val Loss: 0.2575\nEarlyStopper counter: 12 out of 15\nEpoch 35/50 [x->c Combined] - Train Loss: 0.2438, Val Loss: 0.2576\nEarlyStopper counter: 13 out of 15\nEpoch 36/50 [x->c Combined] - Train Loss: 0.2546, Val Loss: 0.2534\nEarlyStopper counter: 14 out of 15\nEpoch 37/50 [x->c Combined] - Train Loss: 0.2472, Val Loss: 0.2499\nEarlyStopper counter: 15 out of 15\nEarly stopping at epoch 37 for x->c combined training.\nLoaded best model state from early stopping.\n--- Stage 2 (Combined Model): Training c -> y (Labels from Predicted & LLM Concepts) ---\nEpoch 1/50 [c->y Combined] - Train Loss: 0.5971, Val Loss: 0.5116\nValidation score decreased (inf --> 0.511588).  Saving model ...\nEpoch 2/50 [c->y Combined] - Train Loss: 0.4803, Val Loss: 0.5109\nValidation score decreased (0.511588 --> 0.510882).  Saving model ...\nEpoch 3/50 [c->y Combined] - Train Loss: 0.4701, Val Loss: 0.5033\nValidation score decreased (0.510882 --> 0.503280).  Saving model ...\nEpoch 4/50 [c->y Combined] - Train Loss: 0.4591, Val Loss: 0.5037\nEarlyStopper counter: 1 out of 15\nEpoch 5/50 [c->y Combined] - Train Loss: 0.4568, Val Loss: 0.4962\nValidation score decreased (0.503280 --> 0.496228).  Saving model ...\nEpoch 6/50 [c->y Combined] - Train Loss: 0.4567, Val Loss: 0.4961\nValidation score decreased (0.496228 --> 0.496051).  Saving model ...\nEpoch 7/50 [c->y Combined] - Train Loss: 0.4564, Val Loss: 0.4909\nValidation score decreased (0.496051 --> 0.490899).  Saving model ...\nEpoch 8/50 [c->y Combined] - Train Loss: 0.4469, Val Loss: 0.4892\nValidation score decreased (0.490899 --> 0.489165).  Saving model ...\nEpoch 9/50 [c->y Combined] - Train Loss: 0.4479, Val Loss: 0.4940\nEarlyStopper counter: 1 out of 15\nEpoch 10/50 [c->y Combined] - Train Loss: 0.4695, Val Loss: 0.4880\nValidation score decreased (0.489165 --> 0.487978).  Saving model ...\nEpoch 11/50 [c->y Combined] - Train Loss: 0.4614, Val Loss: 0.5189\nEarlyStopper counter: 1 out of 15\nEpoch 12/50 [c->y Combined] - Train Loss: 0.4513, Val Loss: 0.4872\nValidation score decreased (0.487978 --> 0.487165).  Saving model ...\nEpoch 13/50 [c->y Combined] - Train Loss: 0.4593, Val Loss: 0.4910\nEarlyStopper counter: 1 out of 15\nEpoch 14/50 [c->y Combined] - Train Loss: 0.4605, Val Loss: 0.4913\nEarlyStopper counter: 2 out of 15\nEpoch 15/50 [c->y Combined] - Train Loss: 0.4612, Val Loss: 0.4913\nEarlyStopper counter: 3 out of 15\nEpoch 16/50 [c->y Combined] - Train Loss: 0.4564, Val Loss: 0.4914\nEarlyStopper counter: 4 out of 15\nEpoch 17/50 [c->y Combined] - Train Loss: 0.4576, Val Loss: 0.4902\nEarlyStopper counter: 5 out of 15\nEpoch 18/50 [c->y Combined] - Train Loss: 0.4546, Val Loss: 0.5001\nEarlyStopper counter: 6 out of 15\nEpoch 19/50 [c->y Combined] - Train Loss: 0.4596, Val Loss: 0.5084\nEarlyStopper counter: 7 out of 15\nEpoch 20/50 [c->y Combined] - Train Loss: 0.4747, Val Loss: 0.4920\nEarlyStopper counter: 8 out of 15\nEpoch 21/50 [c->y Combined] - Train Loss: 0.4457, Val Loss: 0.4891\nEarlyStopper counter: 9 out of 15\nEpoch 22/50 [c->y Combined] - Train Loss: 0.4445, Val Loss: 0.4858\nValidation score decreased (0.487165 --> 0.485770).  Saving model ...\nEpoch 23/50 [c->y Combined] - Train Loss: 0.4439, Val Loss: 0.5233\nEarlyStopper counter: 1 out of 15\nEpoch 24/50 [c->y Combined] - Train Loss: 0.4540, Val Loss: 0.4917\nEarlyStopper counter: 2 out of 15\nEpoch 25/50 [c->y Combined] - Train Loss: 0.4712, Val Loss: 0.4956\nEarlyStopper counter: 3 out of 15\nEpoch 26/50 [c->y Combined] - Train Loss: 0.4550, Val Loss: 0.4898\nEarlyStopper counter: 4 out of 15\nEpoch 27/50 [c->y Combined] - Train Loss: 0.4575, Val Loss: 0.4928\nEarlyStopper counter: 5 out of 15\nEpoch 28/50 [c->y Combined] - Train Loss: 0.4638, Val Loss: 0.5346\nEarlyStopper counter: 6 out of 15\nEpoch 29/50 [c->y Combined] - Train Loss: 0.4745, Val Loss: 0.4889\nEarlyStopper counter: 7 out of 15\nEpoch 30/50 [c->y Combined] - Train Loss: 0.4411, Val Loss: 0.4874\nEarlyStopper counter: 8 out of 15\nEpoch 31/50 [c->y Combined] - Train Loss: 0.4518, Val Loss: 0.4985\nEarlyStopper counter: 9 out of 15\nEpoch 32/50 [c->y Combined] - Train Loss: 0.4493, Val Loss: 0.4867\nEarlyStopper counter: 10 out of 15\nEpoch 33/50 [c->y Combined] - Train Loss: 0.4473, Val Loss: 0.4855\nValidation score decreased (0.485770 --> 0.485538).  Saving model ...\nEpoch 34/50 [c->y Combined] - Train Loss: 0.4502, Val Loss: 0.4881\nEarlyStopper counter: 1 out of 15\nEpoch 35/50 [c->y Combined] - Train Loss: 0.4456, Val Loss: 0.4965\nEarlyStopper counter: 2 out of 15\nEpoch 36/50 [c->y Combined] - Train Loss: 0.4443, Val Loss: 0.4916\nEarlyStopper counter: 3 out of 15\nEpoch 37/50 [c->y Combined] - Train Loss: 0.4643, Val Loss: 0.5190\nEarlyStopper counter: 4 out of 15\nEpoch 38/50 [c->y Combined] - Train Loss: 0.4452, Val Loss: 0.4836\nValidation score decreased (0.485538 --> 0.483578).  Saving model ...\nEpoch 39/50 [c->y Combined] - Train Loss: 0.4438, Val Loss: 0.4936\nEarlyStopper counter: 1 out of 15\nEpoch 40/50 [c->y Combined] - Train Loss: 0.4489, Val Loss: 0.4876\nEarlyStopper counter: 2 out of 15\nEpoch 41/50 [c->y Combined] - Train Loss: 0.4524, Val Loss: 0.5333\nEarlyStopper counter: 3 out of 15\nEpoch 42/50 [c->y Combined] - Train Loss: 0.4554, Val Loss: 0.4883\nEarlyStopper counter: 4 out of 15\nEpoch 43/50 [c->y Combined] - Train Loss: 0.4516, Val Loss: 0.4912\nEarlyStopper counter: 5 out of 15\nEpoch 44/50 [c->y Combined] - Train Loss: 0.4609, Val Loss: 0.4832\nValidation score decreased (0.483578 --> 0.483199).  Saving model ...\nEpoch 45/50 [c->y Combined] - Train Loss: 0.4573, Val Loss: 0.4902\nEarlyStopper counter: 1 out of 15\nEpoch 46/50 [c->y Combined] - Train Loss: 0.4556, Val Loss: 0.4931\nEarlyStopper counter: 2 out of 15\nEpoch 47/50 [c->y Combined] - Train Loss: 0.4486, Val Loss: 0.4897\nEarlyStopper counter: 3 out of 15\nEpoch 48/50 [c->y Combined] - Train Loss: 0.4493, Val Loss: 0.4897\nEarlyStopper counter: 4 out of 15\nEpoch 49/50 [c->y Combined] - Train Loss: 0.4408, Val Loss: 0.4894\nEarlyStopper counter: 5 out of 15\nEpoch 50/50 [c->y Combined] - Train Loss: 0.4453, Val Loss: 0.4879\nEarlyStopper counter: 6 out of 15\nLoaded best model state from early stopping.\nMultiLabelNN2 (Context-Aware CBM) trained sequentially.\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"print(\"--- Evaluating Sequentially Trained MultiLabelNN1 (Standard CBM) ---\")\n# Use the existing test_model function (defined in Cell 6 of your notebook)\nc_true_seq, y_true_seq, binary_c_test_pred_seq, continuous_c_test_pred_seq, y_pred_seq = test_model(\n    model1_sequential, \n    test_loader, \n    binary_concept_idx\n)\n\nprint(\"\\n--- Evaluating Sequentially Trained MultiLabelNN2 (Context-Aware CBM) ---\")\n# Use the existing test_combined_model function (defined in Cell 6 of your notebook)\nc_true_llm_seq, y_true_llm_seq, binary_c_test_pred_llm_seq, continuous_c_test_pred_llm_seq, y_pred_llm_seq = test_combined_model(\n    model2_sequential, \n    test_loader, \n    binary_concept_idx\n)\n\n# Get concept labels (same as before)\nconcept_labels = processor.get_vanilla_concepts()\n\nprint(\"\\n--- Concept Evaluation for Sequentially Trained MultiLabelNN1 (Standard CBM) ---\")\n# Use the existing evaluate_concept_predictor function (defined in Cell 6 of your notebook)\nvanilla_c_results_seq = evaluate_concept_predictor(\n    ground_truth_c=c_true_seq, \n    binary_predictions=binary_c_test_pred_seq, \n    continuous_predictions=continuous_c_test_pred_seq, \n    concept_labels=concept_labels, \n    binary_concept_idx=binary_concept_idx\n)\n\nprint(\"\\n--- Concept Evaluation for Sequentially Trained MultiLabelNN2 (Context-Aware CBM) ---\")\n# Use the existing evaluate_concept_predictor function (defined in Cell 6 of your notebook)\nllm_c_results_seq = evaluate_concept_predictor(\n    ground_truth_c=c_true_llm_seq, \n    binary_predictions=binary_c_test_pred_llm_seq, \n    continuous_predictions=continuous_c_test_pred_llm_seq, \n    concept_labels=concept_labels, \n    binary_concept_idx=binary_concept_idx\n)\n\nprint(\"\\n--- Label Prediction Evaluation for Sequentially Trained MultiLabelNN1 (Standard CBM) ---\")\n# Use the existing evaluate_label_predictor function (defined in Cell 6 of your notebook)\nlabel_results_model1_seq_df = evaluate_label_predictor(y_true_seq, y_pred_seq)\nprint(label_results_model1_seq_df)\n\nprint(\"\\n--- Label Prediction Evaluation for Sequentially Trained MultiLabelNN2 (Context-Aware CBM) ---\")\n# Use the existing evaluate_label_predictor function (defined in Cell 6 of your notebook)\nlabel_results_model2_seq_df = evaluate_label_predictor(y_true_llm_seq, y_pred_llm_seq)\nprint(label_results_model2_seq_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T06:14:10.985189Z","iopub.execute_input":"2025-05-29T06:14:10.985538Z","iopub.status.idle":"2025-05-29T06:14:11.084511Z","shell.execute_reply.started":"2025-05-29T06:14:10.985507Z","shell.execute_reply":"2025-05-29T06:14:11.083527Z"}},"outputs":[{"name":"stdout","text":"--- Evaluating Sequentially Trained MultiLabelNN1 (Standard CBM) ---\n\n--- Evaluating Sequentially Trained MultiLabelNN2 (Context-Aware CBM) ---\n\n--- Concept Evaluation for Sequentially Trained MultiLabelNN1 (Standard CBM) ---\n{'Label': 'c_sofa_avg_cardiovascular', 'MSE': 0.098, 'MAE': 0.165, 'RMSE': 0.314, 'R2': -0.556}\n{'Label': 'c_sofa_avg_respiration', 'MSE': 0.007, 'MAE': 0.064, 'RMSE': 0.085, 'R2': 0.724}\n{'Label': 'c_sofa_avg_renal', 'MSE': 0.031, 'MAE': 0.121, 'RMSE': 0.175, 'R2': 0.567}\n{'Label': 'c_sofa_avg_cns', 'MSE': 0.005, 'MAE': 0.046, 'RMSE': 0.072, 'R2': -0.07}\n{'Label': 'c_first24hr_sofa_max_cardiovascular', 'MSE': 0.185, 'MAE': 0.324, 'RMSE': 0.43, 'R2': -0.342}\n{'Label': 'c_first24hr_sofa_max_respiration', 'MSE': 0.115, 'MAE': 0.305, 'RMSE': 0.339, 'R2': -0.161}\n{'Label': 'c_first24hr_sofa_max_renal', 'MSE': 0.055, 'MAE': 0.182, 'RMSE': 0.235, 'R2': 0.493}\n{'Label': 'c_first24hr_sofa_max_cns', 'MSE': 0.083, 'MAE': 0.221, 'RMSE': 0.287, 'R2': 0.148}\n{'Label': 'c_sofa_max_cardiovascular', 'MSE': 0.19, 'MAE': 0.274, 'RMSE': 0.436, 'R2': -0.669}\n{'Label': 'c_sofa_max_respiration', 'MSE': 0.029, 'MAE': 0.117, 'RMSE': 0.169, 'R2': 0.672}\n{'Label': 'c_sofa_max_renal', 'MSE': 0.074, 'MAE': 0.206, 'RMSE': 0.272, 'R2': 0.516}\n{'Label': 'c_sofa_max_cns', 'MSE': 0.014, 'MAE': 0.093, 'RMSE': 0.118, 'R2': 0.888}\n{'Label': 'c_svr_resp_comorbidity', 'Precision': 0.969, 'Recall': 0.95, 'F1 Score': 0.96, 'Accuracy': 0.98}\n{'Label': 'c_mod_resp_comorbidity', 'Precision': 0.89, 'Recall': 0.955, 'F1 Score': 0.921, 'Accuracy': 0.89}\n\n--- Concept Evaluation for Sequentially Trained MultiLabelNN2 (Context-Aware CBM) ---\n{'Label': 'c_sofa_avg_cardiovascular', 'MSE': 0.093, 'MAE': 0.162, 'RMSE': 0.306, 'R2': -0.475}\n{'Label': 'c_sofa_avg_respiration', 'MSE': 0.007, 'MAE': 0.064, 'RMSE': 0.082, 'R2': 0.742}\n{'Label': 'c_sofa_avg_renal', 'MSE': 0.037, 'MAE': 0.135, 'RMSE': 0.193, 'R2': 0.475}\n{'Label': 'c_sofa_avg_cns', 'MSE': 0.005, 'MAE': 0.053, 'RMSE': 0.068, 'R2': 0.06}\n{'Label': 'c_first24hr_sofa_max_cardiovascular', 'MSE': 0.176, 'MAE': 0.323, 'RMSE': 0.42, 'R2': -0.277}\n{'Label': 'c_first24hr_sofa_max_respiration', 'MSE': 0.087, 'MAE': 0.231, 'RMSE': 0.295, 'R2': 0.123}\n{'Label': 'c_first24hr_sofa_max_renal', 'MSE': 0.056, 'MAE': 0.176, 'RMSE': 0.237, 'R2': 0.486}\n{'Label': 'c_first24hr_sofa_max_cns', 'MSE': 0.086, 'MAE': 0.223, 'RMSE': 0.294, 'R2': 0.108}\n{'Label': 'c_sofa_max_cardiovascular', 'MSE': 0.168, 'MAE': 0.242, 'RMSE': 0.41, 'R2': -0.472}\n{'Label': 'c_sofa_max_respiration', 'MSE': 0.024, 'MAE': 0.118, 'RMSE': 0.156, 'R2': 0.721}\n{'Label': 'c_sofa_max_renal', 'MSE': 0.072, 'MAE': 0.209, 'RMSE': 0.269, 'R2': 0.528}\n{'Label': 'c_sofa_max_cns', 'MSE': 0.013, 'MAE': 0.087, 'RMSE': 0.112, 'R2': 0.899}\n{'Label': 'c_svr_resp_comorbidity', 'Precision': 0.97, 'Recall': 0.96, 'F1 Score': 0.965, 'Accuracy': 0.982}\n{'Label': 'c_mod_resp_comorbidity', 'Precision': 0.894, 'Recall': 0.955, 'F1 Score': 0.923, 'Accuracy': 0.893}\n\n--- Label Prediction Evaluation for Sequentially Trained MultiLabelNN1 (Standard CBM) ---\n         Precision    Recall  F1 Score       AUC  Accuracy\nMetrics   0.686099  0.735577  0.709977  0.676532  0.680307\n\n--- Label Prediction Evaluation for Sequentially Trained MultiLabelNN2 (Context-Aware CBM) ---\n         Precision    Recall  F1 Score       AUC  Accuracy\nMetrics   0.773504  0.870192  0.819005  0.790287  0.795396\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"Vanilla\n{'Label': 'c_sofa_avg_cardiovascular', 'MSE': 0.271, 'MAE': 0.19, 'RMSE': 0.521, 'R2': -3.288}\n{'Label': 'c_sofa_avg_respiration', 'MSE': 0.129, 'MAE': 0.271, 'RMSE': 0.359, 'R2': -3.911}\n{'Label': 'c_sofa_avg_renal', 'MSE': 1.864, 'MAE': 0.745, 'RMSE': 1.365, 'R2': -25.232}\n{'Label': 'c_sofa_avg_cns', 'MSE': 0.02, 'MAE': 0.104, 'RMSE': 0.14, 'R2': -3.042}\n{'Label': 'c_first24hr_sofa_max_cardiovascular', 'MSE': 0.421, 'MAE': 0.336, 'RMSE': 0.649, 'R2': -2.045}\n{'Label': 'c_first24hr_sofa_max_respiration', 'MSE': 0.16, 'MAE': 0.324, 'RMSE': 0.4, 'R2': -0.61}\n{'Label': 'c_first24hr_sofa_max_renal', 'MSE': 0.112, 'MAE': 0.275, 'RMSE': 0.335, 'R2': -0.031}\n{'Label': 'c_first24hr_sofa_max_cns', 'MSE': 0.171, 'MAE': 0.242, 'RMSE': 0.413, 'R2': -0.763}\n{'Label': 'c_sofa_max_cardiovascular', 'MSE': 0.653, 'MAE': 0.282, 'RMSE': 0.808, 'R2': -4.729}\n{'Label': 'c_sofa_max_respiration', 'MSE': 0.043, 'MAE': 0.151, 'RMSE': 0.206, 'R2': 0.511}\n{'Label': 'c_sofa_max_renal', 'MSE': 0.26, 'MAE': 0.405, 'RMSE': 0.51, 'R2': -0.696}\n{'Label': 'c_sofa_max_cns', 'MSE': 0.026, 'MAE': 0.129, 'RMSE': 0.161, 'R2': 0.791}\n{'Label': 'c_svr_resp_comorbidity', 'Precision': 0.969, 'Recall': 0.95, 'F1 Score': 0.96, 'Accuracy': 0.98}\n{'Label': 'c_mod_resp_comorbidity', 'Precision': 0.771, 'Recall': 0.955, 'F1 Score': 0.853, 'Accuracy': 0.777}\nLLM\n{'Label': 'c_sofa_avg_cardiovascular', 'MSE': 0.261, 'MAE': 0.174, 'RMSE': 0.511, 'R2': -3.129}\n{'Label': 'c_sofa_avg_respiration', 'MSE': 0.009, 'MAE': 0.073, 'RMSE': 0.094, 'R2': 0.665}\n{'Label': 'c_sofa_avg_renal', 'MSE': 0.052, 'MAE': 0.159, 'RMSE': 0.228, 'R2': 0.268}\n{'Label': 'c_sofa_avg_cns', 'MSE': 0.005, 'MAE': 0.051, 'RMSE': 0.067, 'R2': 0.069}\n{'Label': 'c_first24hr_sofa_max_cardiovascular', 'MSE': 0.434, 'MAE': 0.347, 'RMSE': 0.659, 'R2': -2.145}\n{'Label': 'c_first24hr_sofa_max_respiration', 'MSE': 0.11, 'MAE': 0.269, 'RMSE': 0.332, 'R2': -0.111}\n{'Label': 'c_first24hr_sofa_max_renal', 'MSE': 0.648, 'MAE': 0.393, 'RMSE': 0.805, 'R2': -4.941}\n{'Label': 'c_first24hr_sofa_max_cns', 'MSE': 0.096, 'MAE': 0.256, 'RMSE': 0.31, 'R2': 0.007}\n{'Label': 'c_sofa_max_cardiovascular', 'MSE': 0.55, 'MAE': 0.345, 'RMSE': 0.742, 'R2': -3.827}\n{'Label': 'c_sofa_max_respiration', 'MSE': 0.068, 'MAE': 0.141, 'RMSE': 0.261, 'R2': 0.216}\n{'Label': 'c_sofa_max_renal', 'MSE': 0.171, 'MAE': 0.215, 'RMSE': 0.414, 'R2': -0.117}\n{'Label': 'c_sofa_max_cns', 'MSE': 0.038, 'MAE': 0.139, 'RMSE': 0.195, 'R2': 0.693}\n{'Label': 'c_svr_resp_comorbidity', 'Precision': 0.97, 'Recall': 0.96, 'F1 Score': 0.965, 'Accuracy': 0.982}\n{'Label': 'c_mod_resp_comorbidity', 'Precision': 0.894, 'Recall': 0.955, 'F1 Score': 0.923, 'Accuracy': 0.893}\n         Precision    Recall  F1 Score       AUC  Accuracy\nMetrics   0.740113  0.629808  0.680519  0.689221  0.685422\n         Precision    Recall  F1 Score       AUC  Accuracy\nMetrics   0.771784  0.894231  0.828508  0.796842  0.803069","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T06:14:11.086057Z","iopub.execute_input":"2025-05-29T06:14:11.086400Z","iopub.status.idle":"2025-05-29T06:14:11.102144Z","shell.execute_reply.started":"2025-05-29T06:14:11.086376Z","shell.execute_reply":"2025-05-29T06:14:11.100798Z"}},"outputs":[{"traceback":["\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_35/3480086513.py\"\u001b[0;36m, line \u001b[0;32m31\u001b[0m\n\u001b[0;31m    Precision    Recall  F1 Score       AUC  Accuracy\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"],"ename":"IndentationError","evalue":"unexpected indent (3480086513.py, line 31)","output_type":"error"}],"execution_count":15},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Get predictions from sequentially trained model1_sequential ---\nprint(\"Getting predictions from sequentially trained model1_sequential...\")\nc_true_m1_seq, y_true_m1_seq, binary_c_test_pred_m1_seq, cont_c_test_pred_m1_seq, y_pred_m1_seq_raw = test_model(\n    model1_sequential, \n    test_loader, \n    binary_concept_idx\n)\n\n# Concatenate batch predictions for model1_sequential\nbinary_c_pred_m1_seq_full = np.concatenate(binary_c_test_pred_m1_seq, axis=0)\ncontinuous_c_pred_m1_seq_full = np.concatenate(cont_c_test_pred_m1_seq, axis=0)\nc_pred_m1_seq = np.concatenate([continuous_c_pred_m1_seq_full, binary_c_pred_m1_seq_full], axis=1) # Order: continuous then binary for c_pred\n# Note: Ensure the order matches how test_concepts is structured (continuous then binary).\n# Based on your MIMICDataProcessor: self.C_tensor_test was np.concatenate([C_test_scaled, C_bin_test.values.astype(float)], axis=1)\n# And binary_concept_idx marks the start of binary concepts.\n# The test_model returns binary_c_pred and continuous_c_pred separately.\n# If test_concepts is [C_cont_test, C_bin_test], then:\n# c_pred_m1_seq needs to be [continuous_c_pred_m1_seq_full, binary_c_pred_m1_seq_full]\n# However, your original c_pred (Cell 25) was [binary_c_pred, continuous_c_pred].\n# Let's stick to your original notebook's convention for c_pred: [binary, continuous]\n# The evaluate_concept_predictor expects this based on its indexing: `c[:, i - 12]` for binary.\n# This means binary concepts (2) come first, then continuous (12).\n# So, when using in evaluate_concept_predictor or interventions, ensure indices are correct.\n# For consistency with original intervention code (e.g. b = b[-2:] for binary), we'll assume c_pred is [continuous, binary]\n# And binary_concept_idx will be [12, 13] relative to the 14 vanilla concepts.\n\n# Let's re-verify the order of c_pred in your original notebook (Cell 25):\n# c_pred = np.concatenate([binary_c_pred, continuous_c_pred], axis=1)\n# This means binary concepts are indeed first in c_pred.\n# test_model returns binary_c_pred, continuous_c_pred.\n# So c_pred_m1_seq should be:\nc_pred_m1_seq = np.concatenate([binary_c_pred_m1_seq_full, continuous_c_pred_m1_seq_full], axis=1)\n\n\ny_pred_final_m1_seq = np.concatenate(y_pred_m1_seq_raw, axis=0).reshape(-1)\ny_true_final_m1_seq = np.concatenate(y_true_m1_seq, axis=0).reshape(-1)\n\n# --- Get predictions from sequentially trained model2_sequential ---\nprint(\"Getting predictions from sequentially trained model2_sequential...\")\nc_true_m2_seq, y_true_m2_seq, binary_c_test_pred_m2_seq, cont_c_test_pred_m2_seq, y_pred_m2_seq_raw = test_combined_model(\n    model2_sequential, \n    test_loader, \n    binary_concept_idx\n)\n\n# Concatenate batch predictions for model2_sequential\nbinary_c_pred_m2_seq_full = np.concatenate(binary_c_test_pred_m2_seq, axis=0)\ncontinuous_c_pred_m2_seq_full = np.concatenate(cont_c_test_pred_m2_seq, axis=0)\n# Following original notebook's c_pred_llm structure:\nc_pred_m2_seq = np.concatenate([binary_c_pred_m2_seq_full, continuous_c_pred_m2_seq_full], axis=1)\n\ny_pred_final_m2_seq = np.concatenate(y_pred_m2_seq_raw, axis=0).reshape(-1)\ny_true_final_m2_seq = np.concatenate(y_true_m2_seq, axis=0).reshape(-1)\n\n\n# --- Define FP/FN indices for sequentially trained models ---\nthreshold = 0.5\n\n# For model1_sequential\npredicted_labels_m1_seq = (y_pred_final_m1_seq >= threshold).astype(int)\nfp_idx_m1_seq = np.where((y_true_final_m1_seq == 0) & (predicted_labels_m1_seq == 1))[0]\nfn_idx_m1_seq = np.where((y_true_final_m1_seq == 1) & (predicted_labels_m1_seq == 0))[0]\n\n# For model2_sequential\npredicted_labels_m2_seq = (y_pred_final_m2_seq >= threshold).astype(int)\nfp_idx_m2_seq = np.where((y_true_final_m2_seq == 0) & (predicted_labels_m2_seq == 1))[0]\nfn_idx_m2_seq = np.where((y_true_final_m2_seq == 1) & (predicted_labels_m2_seq == 0))[0]\n\n# --- Prepare test_features, test_concepts, test_llm_concepts (as in Cell 26 of original) ---\n# This part is independent of model training and should be the same\n_test_features_batches = []\n_test_concepts_batches = [] # Ground truth vanilla concepts\n_test_llm_concepts_batches = [] # Ground truth LLM concepts\n\nfor data_batch in test_loader:\n    _test_features_batches.append(data_batch[0].cpu().numpy())\n    _test_concepts_batches.append(data_batch[1].cpu().numpy()) # This is C_tensor_test\n    _test_llm_concepts_batches.append(data_batch[2].cpu().numpy()) # This is LLM_C_tensor_test\n\ntest_features = np.concatenate(_test_features_batches, axis=0)\ntest_concepts = np.concatenate(_test_concepts_batches, axis=0) # Ground truth vanilla concepts C_tensor_test\ntest_llm_concepts = np.concatenate(_test_llm_concepts_batches, axis=0) # Ground truth LLM concepts\n\n# Confirm structure of test_concepts. From MIMICDataProcessor:\n# C_tensor_test = torch.tensor(C_test_full, dtype=torch.float32)\n# C_test_full = np.concatenate([C_test_scaled, C_bin_test.values.astype(float)], axis=1)\n# This means continuous concepts (12) are first, then binary concepts (2) in test_concepts.\n# binary_concept_idx correctly identifies indices [12, 13] in test_concepts.\n\n# IMPORTANT: The c_pred_m1_seq and c_pred_m2_seq were created as [binary_pred, continuous_pred].\n# The intervention code in the original notebook that uses c_pred (e.g. `b = c_pred[a].copy(); b = b[-2:]`)\n# implies that for those specific loops, it expects binary concepts to be at the end of the slice.\n# Let's adjust c_pred_m1_seq and c_pred_m2_seq to be [continuous_pred, binary_pred]\n# to match the structure of test_concepts (ground truth) for easier indexing in interventions.\n# This also means the evaluate_concept_predictor indexing `i - 12` for binary predictions will be correct\n# if binary_predictions (the argument) contains only binary concepts and binary_concept_idx is [12,13].\n\n# Re-evaluate c_pred structure for intervention consistency:\n# Original Int1 intervention code: `b = c_pred[a].copy(); b = b[-2:]` implies binary is at the end.\n# test_model and test_combined_model return: binary_c_pred, continuous_c_pred.\n# If c_pred is [binary_c_pred_full, continuous_c_pred_full], then b[-2:] is wrong.\n# If c_pred is [continuous_c_pred_full, binary_c_pred_full], then b[-2:] is correct for binary intervention.\n\n# Let's define c_pred_m1_seq and c_pred_m2_seq to match the structure of test_concepts:\n# [continuous_pred_concepts (12), binary_pred_concepts (2)]\nc_pred_m1_seq = np.concatenate([continuous_c_pred_m1_seq_full, binary_c_pred_m1_seq_full], axis=1)\nc_pred_m2_seq = np.concatenate([continuous_c_pred_m2_seq_full, binary_c_pred_m2_seq_full], axis=1)\n\n# Now, when intervening on c_pred_m1_seq or c_pred_m2_seq:\n# - Continuous concepts are at indices 0-11.\n# - Binary concepts are at indices 12-13.\n# The original intervention code (e.g., Cell 28) `b = b[-2:]` for binary intervention on `c_pred` and\n# `b = b[:-2]` for continuous intervention on `c_pred` will now work directly with this structure.\n# The `intervention_concept_idxs` like `[12,13]` or `[0,1,...,11]` will correctly map.\n\nprint(\"Test predictions and FP/FN indices for sequentially trained models are ready.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T06:15:44.424405Z","iopub.execute_input":"2025-05-29T06:15:44.425483Z","iopub.status.idle":"2025-05-29T06:15:44.482007Z","shell.execute_reply.started":"2025-05-29T06:15:44.425446Z","shell.execute_reply":"2025-05-29T06:15:44.480776Z"}},"outputs":[{"name":"stdout","text":"Getting predictions from sequentially trained model1_sequential...\nGetting predictions from sequentially trained model2_sequential...\nTest predictions and FP/FN indices for sequentially trained models are ready.\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"from copy import deepcopy # Already imported in your notebook\n\n# --- Intervention Models Definition (Adapted from Cells 27, 31, 34, 47) ---\n# Ensure these are defined with num_features = x_size (e.g., 21)\n\nclass Int1(nn.Module): # For base CBM, intervening on binary concepts\n    def __init__(self, num_features, num_binary_concepts, num_continuous_concepts, num_labels):\n        super(Int1, self).__init__()\n        self.layer1 = nn.Linear(num_features, num_binary_concepts, bias=False) # Will be frozen or unused\n        self.layer2 = nn.Linear(num_features, num_continuous_concepts, bias=False)\n        self.layer3 = nn.Linear(num_binary_concepts, num_labels, bias=False)\n        self.layer4 = nn.Linear(num_continuous_concepts, num_labels, bias=False)\n        \n    def forward(self, x, binary_int): # x is for layer2 to get continuous_c\n        # binary_c = self.layer1(x) # Original concept prediction is bypassed\n        continuous_c = self.layer2(x)\n        # binary_int is already sigmoid-activated if it's a prediction, or 0/1 if true\n        y_pred = torch.sigmoid(self.layer3(binary_int) + self.layer4(continuous_c))\n        return y_pred\n\nclass Int2(nn.Module): # For base CBM, intervening on continuous concepts\n    def __init__(self, num_features, num_binary_concepts, num_continuous_concepts, num_labels):\n        super(Int2, self).__init__()\n        self.layer1 = nn.Linear(num_features, num_binary_concepts, bias=False)\n        self.layer2 = nn.Linear(num_features, num_continuous_concepts, bias=False) # Will be frozen or unused\n        self.layer3 = nn.Linear(num_binary_concepts, num_labels, bias=False)\n        self.layer4 = nn.Linear(num_continuous_concepts, num_labels, bias=False)\n        \n    def forward(self, x, continuous_int): # x is for layer1 to get binary_c\n        binary_c = self.layer1(x)\n        # continuous_c = self.layer2(x) # Original concept prediction is bypassed\n        binary_c = torch.sigmoid(binary_c)\n        y_pred = torch.sigmoid(self.layer3(binary_c) + self.layer4(continuous_int))\n        return y_pred\n\nclass Int3(nn.Module): # For enhanced CBM, intervening on binary concepts\n    def __init__(self, num_features, num_binary_concepts, num_continuous_concepts, num_llm_concepts, num_labels):\n        super(Int3, self).__init__()\n        self.layer1 = nn.Linear(num_features, num_binary_concepts, bias=False)\n        self.layer2 = nn.Linear(num_features, num_continuous_concepts, bias=False)\n        self.layer3 = nn.Linear(num_binary_concepts, num_labels, bias=False)\n        self.layer4 = nn.Linear(num_continuous_concepts, num_labels, bias=False)\n        self.layer5 = nn.Linear(num_llm_concepts, num_labels, bias=False)\n\n    def forward(self, x, binary_int, llm_c):\n        continuous_c = self.layer2(x)\n        y_pred = torch.sigmoid(self.layer3(binary_int) + self.layer4(continuous_c) + self.layer5(llm_c))\n        return y_pred\n\nclass Int4(nn.Module): # For enhanced CBM, intervening on continuous concepts\n    def __init__(self, num_features, num_binary_concepts, num_continuous_concepts, num_llm_concepts, num_labels):\n        super(Int4, self).__init__()\n        self.layer1 = nn.Linear(num_features, num_binary_concepts, bias=False)\n        self.layer2 = nn.Linear(num_features, num_continuous_concepts, bias=False)\n        self.layer3 = nn.Linear(num_binary_concepts, num_labels, bias=False)\n        self.layer4 = nn.Linear(num_continuous_concepts, num_labels, bias=False)\n        self.layer5 = nn.Linear(num_llm_concepts, num_labels, bias=False)\n\n    def forward(self, x, continuous_int, llm_c):\n        binary_c = torch.sigmoid(self.layer1(x))\n        y_pred = torch.sigmoid(self.layer3(binary_c) + self.layer4(continuous_int) + self.layer5(llm_c))\n        return y_pred\n\nclass Int5(nn.Module): # For enhanced CBM, intervening on LLM concepts\n    def __init__(self, num_features, num_binary_concepts, num_continuous_concepts, num_llm_concepts, num_labels):\n        super(Int5, self).__init__()\n        self.layer1 = nn.Linear(num_features, num_binary_concepts, bias=False)\n        self.layer2 = nn.Linear(num_features, num_continuous_concepts, bias=False)\n        self.layer3 = nn.Linear(num_binary_concepts, num_labels, bias=False)\n        self.layer4 = nn.Linear(num_continuous_concepts, num_labels, bias=False)\n        self.layer5 = nn.Linear(num_llm_concepts, num_labels, bias=False)\n\n    def forward(self, x, llm_int):\n        binary_c = torch.sigmoid(self.layer1(x))\n        continuous_c = self.layer2(x)\n        y_pred = torch.sigmoid(self.layer3(binary_c) + self.layer4(continuous_c) + self.layer5(llm_int))\n        return y_pred\n        \nclass Int6(nn.Module): # For enhanced CBM, intervening on all concept types\n    def __init__(self, num_features, num_binary_concepts, num_continuous_concepts, num_llm_concepts, num_labels):\n        super(Int6, self).__init__()\n        # These layers will effectively be from the c->y part of the model\n        self.layer1 = nn.Linear(num_features, num_binary_concepts, bias=False)    # Not directly used for x->c_bin if binary_int provided\n        self.layer2 = nn.Linear(num_features, num_continuous_concepts, bias=False) # Not directly used for x->c_cont if continuous_int provided\n        self.layer3 = nn.Linear(num_binary_concepts, num_labels, bias=False)\n        self.layer4 = nn.Linear(num_continuous_concepts, num_labels, bias=False)\n        self.layer5 = nn.Linear(num_llm_concepts, num_labels, bias=False)\n\n    def forward(self, x, binary_int, continuous_int, llm_int): # x is technically not needed if all concepts are intervened\n        # binary_c = self.layer1(x) # Bypassed\n        # continuous_c = self.layer2(x) # Bypassed\n        y_pred = torch.sigmoid(self.layer3(binary_int) + self.layer4(continuous_int) + self.layer5(llm_int))\n        return y_pred\n\n# --- Instantiate and Load Weights for Intervention Models (Sequential) ---\nx_size = processor.X_tensor_scaled_train.shape[1] # Should be 21\nnum_binary_concepts = processor.C_bin.shape[1]    # Should be 2\nnum_continuous_concepts = processor.C_cont.shape[1] # Should be 12\nnum_llm_concepts = processor.LLM_C.shape[1]       # Should be 8\nnum_labels = 1\n\n# For model1_sequential (base CBM)\nmodel_int1_seq = Int1(x_size, num_binary_concepts, num_continuous_concepts, num_labels).to(device)\nfor p_new, p_orig in zip(model_int1_seq.parameters(), model1_sequential.parameters()):\n    p_new.data = deepcopy(p_orig.data)\nmodel_int1_seq.eval()\n\nmodel_int2_seq = Int2(x_size, num_binary_concepts, num_continuous_concepts, num_labels).to(device)\nfor p_new, p_orig in zip(model_int2_seq.parameters(), model1_sequential.parameters()):\n    p_new.data = deepcopy(p_orig.data)\nmodel_int2_seq.eval()\n\n# For model2_sequential (enhanced CBM)\nmodel_int3_seq = Int3(x_size, num_binary_concepts, num_continuous_concepts, num_llm_concepts, num_labels).to(device)\nfor p_new, p_orig in zip(model_int3_seq.parameters(), model2_sequential.parameters()):\n    p_new.data = deepcopy(p_orig.data)\nmodel_int3_seq.eval()\n\nmodel_int4_seq = Int4(x_size, num_binary_concepts, num_continuous_concepts, num_llm_concepts, num_labels).to(device)\nfor p_new, p_orig in zip(model_int4_seq.parameters(), model2_sequential.parameters()):\n    p_new.data = deepcopy(p_orig.data)\nmodel_int4_seq.eval()\n\nmodel_int5_seq = Int5(x_size, num_binary_concepts, num_continuous_concepts, num_llm_concepts, num_labels).to(device)\nfor p_new, p_orig in zip(model_int5_seq.parameters(), model2_sequential.parameters()):\n    p_new.data = deepcopy(p_orig.data)\nmodel_int5_seq.eval()\n\nmodel_int6_seq = Int6(x_size, num_binary_concepts, num_continuous_concepts, num_llm_concepts, num_labels).to(device)\nfor p_new, p_orig in zip(model_int6_seq.parameters(), model2_sequential.parameters()):\n    p_new.data = deepcopy(p_orig.data)\nmodel_int6_seq.eval()\n\nprint(\"Intervention models initialized and weights copied from sequentially trained models.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T06:15:59.306987Z","iopub.execute_input":"2025-05-29T06:15:59.307323Z","iopub.status.idle":"2025-05-29T06:15:59.342956Z","shell.execute_reply.started":"2025-05-29T06:15:59.307298Z","shell.execute_reply":"2025-05-29T06:15:59.341638Z"}},"outputs":[{"name":"stdout","text":"Intervention models initialized and weights copied from sequentially trained models.\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"import numpy as np\nimport torch\n\n# Ensure binary_concept_idx refers to indices within the 14 vanilla concepts.\n# If C_cont has 12 features, binary_concept_idx = [12, 13]\n# num_continuous_concepts = processor.C_cont.shape[1] # Should be 12\n# binary_intervention_indices_in_vanilla = list(range(num_continuous_concepts, num_continuous_concepts + num_binary_concepts))\n# continuous_intervention_indices_in_vanilla = list(range(num_continuous_concepts))\n\n# For clarity, let's define these based on your processor\nnum_total_vanilla_concepts = processor.C_tensor_train.shape[1] # Should be 14\ncontinuous_intervention_indices_in_vanilla = list(range(processor.C_cont.shape[1])) # Indices 0-11\nbinary_intervention_indices_in_vanilla = list(range(processor.C_cont.shape[1], num_total_vanilla_concepts)) # Indices 12-13\n\n# --- Interventions on Base CBM (model1_sequential) ---\n\nprint(\"\\n###########################################################################\")\nprint(\"--- Interventions on Base CBM (model1_sequential) ---\")\nprint(\"###########################################################################\")\n\n# --- Binary Concepts Interventions for model1_sequential (Adapted from Cells 28, 29, 30) ---\n\n# Intervention Type: Replacement with True Values\nprint(\"\\n--- Binary Concepts: Replacement with True Values (model1_sequential) ---\")\nfor int_idx_in_vanilla in binary_intervention_indices_in_vanilla:\n    count_fn_corrected, total_fn = 0, 0\n    for a in fn_idx_m1_seq:\n        intervened_vanilla_concepts = c_pred_m1_seq[a].copy()\n        intervened_vanilla_concepts[int_idx_in_vanilla] = test_concepts[a][int_idx_in_vanilla]\n        binary_slice_for_int_model = intervened_vanilla_concepts[binary_intervention_indices_in_vanilla]\n        with torch.no_grad():\n            p_int = model_int1_seq(torch.tensor(test_features[a:a+1]).float().to(device),\n                                   torch.tensor([binary_slice_for_int_model]).float().to(device))\n        if p_int.item() >= 0.5: count_fn_corrected += 1\n        total_fn += 1\n    print(f\"FNs: Intervened on vanilla_concept_idx {int_idx_in_vanilla}: Corrections: {count_fn_corrected/total_fn if total_fn > 0 else 'N/A'} ({count_fn_corrected}/{total_fn})\")\n\n    count_fp_corrected, total_fp = 0, 0\n    for a in fp_idx_m1_seq:\n        intervened_vanilla_concepts = c_pred_m1_seq[a].copy()\n        intervened_vanilla_concepts[int_idx_in_vanilla] = test_concepts[a][int_idx_in_vanilla]\n        binary_slice_for_int_model = intervened_vanilla_concepts[binary_intervention_indices_in_vanilla]\n        with torch.no_grad():\n            p_int = model_int1_seq(torch.tensor(test_features[a:a+1]).float().to(device),\n                                   torch.tensor([binary_slice_for_int_model]).float().to(device))\n        if p_int.item() < 0.5: count_fp_corrected += 1\n        total_fp += 1\n    print(f\"FPs: Intervened on vanilla_concept_idx {int_idx_in_vanilla}: Corrections: {count_fp_corrected/total_fp if total_fp > 0 else 'N/A'} ({count_fp_corrected}/{total_fp})\")\n\n# All binary concepts together with true values\ncount_fn_corrected, total_fn = 0, 0\nfor a in fn_idx_m1_seq:\n    intervened_vanilla_concepts = c_pred_m1_seq[a].copy()\n    for bc_idx in binary_intervention_indices_in_vanilla:\n        intervened_vanilla_concepts[bc_idx] = test_concepts[a][bc_idx]\n    binary_slice_for_int_model = intervened_vanilla_concepts[binary_intervention_indices_in_vanilla]\n    with torch.no_grad():\n        p_int = model_int1_seq(torch.tensor(test_features[a:a+1]).float().to(device),\n                               torch.tensor([binary_slice_for_int_model]).float().to(device))\n    if p_int.item() >= 0.5: count_fn_corrected += 1\n    total_fn += 1\nprint(f\"FNs: All Binary Concepts (True): Corrections: {count_fn_corrected/total_fn if total_fn > 0 else 'N/A'} ({count_fn_corrected}/{total_fn})\")\n\ncount_fp_corrected, total_fp = 0, 0\nfor a in fp_idx_m1_seq:\n    intervened_vanilla_concepts = c_pred_m1_seq[a].copy()\n    for bc_idx in binary_intervention_indices_in_vanilla:\n        intervened_vanilla_concepts[bc_idx] = test_concepts[a][bc_idx]\n    binary_slice_for_int_model = intervened_vanilla_concepts[binary_intervention_indices_in_vanilla]\n    with torch.no_grad():\n        p_int = model_int1_seq(torch.tensor(test_features[a:a+1]).float().to(device),\n                               torch.tensor([binary_slice_for_int_model]).float().to(device))\n    if p_int.item() < 0.5: count_fp_corrected += 1\n    total_fp += 1\nprint(f\"FPs: All Binary Concepts (True): Corrections: {count_fp_corrected/total_fp if total_fp > 0 else 'N/A'} ({count_fp_corrected}/{total_fp})\")\n\n\n# Intervention Type: Replacement with Mean Values\nprint(\"\\n--- Binary Concepts: Replacement with Mean Values (model1_sequential) ---\")\nmean_true_binary_concepts = np.mean(test_concepts[:, binary_intervention_indices_in_vanilla], axis=0)\nfor i, int_idx_in_vanilla in enumerate(binary_intervention_indices_in_vanilla):\n    mean_val_for_concept = mean_true_binary_concepts[i]\n    count_fn_corrected, total_fn = 0, 0\n    for a in fn_idx_m1_seq:\n        intervened_vanilla_concepts = c_pred_m1_seq[a].copy()\n        intervened_vanilla_concepts[int_idx_in_vanilla] = mean_val_for_concept\n        binary_slice_for_int_model = intervened_vanilla_concepts[binary_intervention_indices_in_vanilla]\n        with torch.no_grad():\n            p_int = model_int1_seq(torch.tensor(test_features[a:a+1]).float().to(device),\n                                   torch.tensor([binary_slice_for_int_model]).float().to(device))\n        if p_int.item() >= 0.5: count_fn_corrected += 1\n        total_fn += 1\n    print(f\"FNs: Intervened on vanilla_concept_idx {int_idx_in_vanilla} with Mean: Corrections: {count_fn_corrected/total_fn if total_fn > 0 else 'N/A'} ({count_fn_corrected}/{total_fn})\")\n    \n    count_fp_corrected, total_fp = 0, 0\n    for a in fp_idx_m1_seq:\n        intervened_vanilla_concepts = c_pred_m1_seq[a].copy()\n        intervened_vanilla_concepts[int_idx_in_vanilla] = mean_val_for_concept\n        binary_slice_for_int_model = intervened_vanilla_concepts[binary_intervention_indices_in_vanilla]\n        with torch.no_grad():\n            p_int = model_int1_seq(torch.tensor(test_features[a:a+1]).float().to(device),\n                                   torch.tensor([binary_slice_for_int_model]).float().to(device))\n        if p_int.item() < 0.5: count_fp_corrected += 1\n        total_fp += 1\n    print(f\"FPs: Intervened on vanilla_concept_idx {int_idx_in_vanilla} with Mean: Corrections: {count_fp_corrected/total_fp if total_fp > 0 else 'N/A'} ({count_fp_corrected}/{total_fp})\")\n\n# All binary concepts together with mean values\ncount_fn_corrected, total_fn = 0, 0\nfor a in fn_idx_m1_seq:\n    intervened_vanilla_concepts = c_pred_m1_seq[a].copy()\n    for i, bc_idx in enumerate(binary_intervention_indices_in_vanilla):\n        intervened_vanilla_concepts[bc_idx] = mean_true_binary_concepts[i]\n    binary_slice_for_int_model = intervened_vanilla_concepts[binary_intervention_indices_in_vanilla]\n    with torch.no_grad():\n        p_int = model_int1_seq(torch.tensor(test_features[a:a+1]).float().to(device),\n                               torch.tensor([binary_slice_for_int_model]).float().to(device))\n    if p_int.item() >= 0.5: count_fn_corrected += 1\n    total_fn += 1\nprint(f\"FNs: All Binary Concepts (Mean): Corrections: {count_fn_corrected/total_fn if total_fn > 0 else 'N/A'} ({count_fn_corrected}/{total_fn})\")\n\ncount_fp_corrected, total_fp = 0, 0\nfor a in fp_idx_m1_seq:\n    intervened_vanilla_concepts = c_pred_m1_seq[a].copy()\n    for i, bc_idx in enumerate(binary_intervention_indices_in_vanilla):\n        intervened_vanilla_concepts[bc_idx] = mean_true_binary_concepts[i]\n    binary_slice_for_int_model = intervened_vanilla_concepts[binary_intervention_indices_in_vanilla]\n    with torch.no_grad():\n        p_int = model_int1_seq(torch.tensor(test_features[a:a+1]).float().to(device),\n                               torch.tensor([binary_slice_for_int_model]).float().to(device))\n    if p_int.item() < 0.5: count_fp_corrected += 1\n    total_fp += 1\nprint(f\"FPs: All Binary Concepts (Mean): Corrections: {count_fp_corrected/total_fp if total_fp > 0 else 'N/A'} ({count_fp_corrected}/{total_fp})\")\n\n\n# Intervention Type: Replacement with Median Values\nprint(\"\\n--- Binary Concepts: Replacement with Median Values (model1_sequential) ---\")\nmedian_true_binary_concepts = np.median(test_concepts[:, binary_intervention_indices_in_vanilla], axis=0)\nfor i, int_idx_in_vanilla in enumerate(binary_intervention_indices_in_vanilla):\n    median_val_for_concept = median_true_binary_concepts[i]\n    count_fn_corrected, total_fn = 0, 0\n    for a in fn_idx_m1_seq:\n        intervened_vanilla_concepts = c_pred_m1_seq[a].copy()\n        intervened_vanilla_concepts[int_idx_in_vanilla] = median_val_for_concept\n        binary_slice_for_int_model = intervened_vanilla_concepts[binary_intervention_indices_in_vanilla]\n        with torch.no_grad():\n            p_int = model_int1_seq(torch.tensor(test_features[a:a+1]).float().to(device),\n                                   torch.tensor([binary_slice_for_int_model]).float().to(device))\n        if p_int.item() >= 0.5: count_fn_corrected += 1\n        total_fn += 1\n    print(f\"FNs: Intervened on vanilla_concept_idx {int_idx_in_vanilla} with Median: Corrections: {count_fn_corrected/total_fn if total_fn > 0 else 'N/A'} ({count_fn_corrected}/{total_fn})\")\n\n    count_fp_corrected, total_fp = 0, 0\n    for a in fp_idx_m1_seq:\n        intervened_vanilla_concepts = c_pred_m1_seq[a].copy()\n        intervened_vanilla_concepts[int_idx_in_vanilla] = median_val_for_concept\n        binary_slice_for_int_model = intervened_vanilla_concepts[binary_intervention_indices_in_vanilla]\n        with torch.no_grad():\n            p_int = model_int1_seq(torch.tensor(test_features[a:a+1]).float().to(device),\n                                   torch.tensor([binary_slice_for_int_model]).float().to(device))\n        if p_int.item() < 0.5: count_fp_corrected += 1\n        total_fp += 1\n    print(f\"FPs: Intervened on vanilla_concept_idx {int_idx_in_vanilla} with Median: Corrections: {count_fp_corrected/total_fp if total_fp > 0 else 'N/A'} ({count_fp_corrected}/{total_fp})\")\n    \n# All binary concepts together with median values\ncount_fn_corrected, total_fn = 0, 0\nfor a in fn_idx_m1_seq:\n    intervened_vanilla_concepts = c_pred_m1_seq[a].copy()\n    for i, bc_idx in enumerate(binary_intervention_indices_in_vanilla):\n        intervened_vanilla_concepts[bc_idx] = median_true_binary_concepts[i]\n    binary_slice_for_int_model = intervened_vanilla_concepts[binary_intervention_indices_in_vanilla]\n    with torch.no_grad():\n        p_int = model_int1_seq(torch.tensor(test_features[a:a+1]).float().to(device),\n                               torch.tensor([binary_slice_for_int_model]).float().to(device))\n    if p_int.item() >= 0.5: count_fn_corrected += 1\n    total_fn += 1\nprint(f\"FNs: All Binary Concepts (Median): Corrections: {count_fn_corrected/total_fn if total_fn > 0 else 'N/A'} ({count_fn_corrected}/{total_fn})\")\n\ncount_fp_corrected, total_fp = 0, 0\nfor a in fp_idx_m1_seq:\n    intervened_vanilla_concepts = c_pred_m1_seq[a].copy()\n    for i, bc_idx in enumerate(binary_intervention_indices_in_vanilla):\n        intervened_vanilla_concepts[bc_idx] = median_true_binary_concepts[i]\n    binary_slice_for_int_model = intervened_vanilla_concepts[binary_intervention_indices_in_vanilla]\n    with torch.no_grad():\n        p_int = model_int1_seq(torch.tensor(test_features[a:a+1]).float().to(device),\n                               torch.tensor([binary_slice_for_int_model]).float().to(device))\n    if p_int.item() < 0.5: count_fp_corrected += 1\n    total_fp += 1\nprint(f\"FPs: All Binary Concepts (Median): Corrections: {count_fp_corrected/total_fp if total_fp > 0 else 'N/A'} ({count_fp_corrected}/{total_fp})\")\n\n\n# --- Continuous Concepts Interventions for model1_sequential (Adapted from Cells 31, 32, 33) ---\n# Using model_int2_seq\n\n# Intervention Type: Replacement with True Values\nprint(\"\\n--- Continuous Concepts: Replacement with True Values (model1_sequential) ---\")\nfor int_idx_in_vanilla in continuous_intervention_indices_in_vanilla: # Indices 0-11\n    count_fn_corrected, total_fn = 0, 0\n    for a in fn_idx_m1_seq:\n        intervened_vanilla_concepts = c_pred_m1_seq[a].copy()\n        intervened_vanilla_concepts[int_idx_in_vanilla] = test_concepts[a][int_idx_in_vanilla]\n        continuous_slice_for_int_model = intervened_vanilla_concepts[continuous_intervention_indices_in_vanilla]\n        with torch.no_grad():\n            p_int = model_int2_seq(torch.tensor(test_features[a:a+1]).float().to(device),\n                                   torch.tensor([continuous_slice_for_int_model]).float().to(device))\n        if p_int.item() >= 0.5: count_fn_corrected += 1\n        total_fn += 1\n    print(f\"FNs: Intervened on vanilla_concept_idx {int_idx_in_vanilla}: Corrections: {count_fn_corrected/total_fn if total_fn > 0 else 'N/A'} ({count_fn_corrected}/{total_fn})\")\n\n    count_fp_corrected, total_fp = 0, 0\n    for a in fp_idx_m1_seq:\n        intervened_vanilla_concepts = c_pred_m1_seq[a].copy()\n        intervened_vanilla_concepts[int_idx_in_vanilla] = test_concepts[a][int_idx_in_vanilla]\n        continuous_slice_for_int_model = intervened_vanilla_concepts[continuous_intervention_indices_in_vanilla]\n        with torch.no_grad():\n            p_int = model_int2_seq(torch.tensor(test_features[a:a+1]).float().to(device),\n                                   torch.tensor([continuous_slice_for_int_model]).float().to(device))\n        if p_int.item() < 0.5: count_fp_corrected += 1\n        total_fp += 1\n    print(f\"FPs: Intervened on vanilla_concept_idx {int_idx_in_vanilla}: Corrections: {count_fp_corrected/total_fp if total_fp > 0 else 'N/A'} ({count_fp_corrected}/{total_fp})\")\n\n# All continuous concepts together with true values (Example for selected indices from your original notebook)\nselected_continuous_for_fn = [2, 3, 5, 10] # Example indices from your original cell 31\ncount_fn_corrected, total_fn = 0, 0\nfor a in fn_idx_m1_seq:\n    intervened_vanilla_concepts = c_pred_m1_seq[a].copy()\n    for cc_idx in selected_continuous_for_fn:\n        if cc_idx in continuous_intervention_indices_in_vanilla:\n             intervened_vanilla_concepts[cc_idx] = test_concepts[a][cc_idx]\n    continuous_slice_for_int_model = intervened_vanilla_concepts[continuous_intervention_indices_in_vanilla]\n    with torch.no_grad():\n        p_int = model_int2_seq(torch.tensor(test_features[a:a+1]).float().to(device),\n                               torch.tensor([continuous_slice_for_int_model]).float().to(device))\n    if p_int.item() >= 0.5: count_fn_corrected += 1\n    total_fn += 1\nprint(f\"FNs: Selected Continuous Concepts {selected_continuous_for_fn} (True): Corrections: {count_fn_corrected/total_fn if total_fn > 0 else 'N/A'} ({count_fn_corrected}/{total_fn})\")\n\nselected_continuous_for_fp = [1, 2, 11] # Example indices\ncount_fp_corrected, total_fp = 0, 0\nfor a in fp_idx_m1_seq:\n    intervened_vanilla_concepts = c_pred_m1_seq[a].copy()\n    for cc_idx in selected_continuous_for_fp:\n        if cc_idx in continuous_intervention_indices_in_vanilla:\n            intervened_vanilla_concepts[cc_idx] = test_concepts[a][cc_idx]\n    continuous_slice_for_int_model = intervened_vanilla_concepts[continuous_intervention_indices_in_vanilla]\n    with torch.no_grad():\n        p_int = model_int2_seq(torch.tensor(test_features[a:a+1]).float().to(device),\n                               torch.tensor([continuous_slice_for_int_model]).float().to(device))\n    if p_int.item() < 0.5: count_fp_corrected += 1\n    total_fp += 1\nprint(f\"FPs: Selected Continuous Concepts {selected_continuous_for_fp} (True): Corrections: {count_fp_corrected/total_fp if total_fp > 0 else 'N/A'} ({count_fp_corrected}/{total_fp})\")\n\n\n# Intervention Type: Replacement with Mean Values\nprint(\"\\n--- Continuous Concepts: Replacement with Mean Values (model1_sequential) ---\")\nmean_true_continuous_concepts = np.mean(test_concepts[:, continuous_intervention_indices_in_vanilla], axis=0)\nfor i, int_idx_in_vanilla in enumerate(continuous_intervention_indices_in_vanilla):\n    mean_val_for_concept = mean_true_continuous_concepts[i]\n    count_fn_corrected, total_fn = 0, 0\n    for a in fn_idx_m1_seq:\n        intervened_vanilla_concepts = c_pred_m1_seq[a].copy()\n        intervened_vanilla_concepts[int_idx_in_vanilla] = mean_val_for_concept\n        continuous_slice_for_int_model = intervened_vanilla_concepts[continuous_intervention_indices_in_vanilla]\n        with torch.no_grad():\n            p_int = model_int2_seq(torch.tensor(test_features[a:a+1]).float().to(device),\n                                   torch.tensor([continuous_slice_for_int_model]).float().to(device))\n        if p_int.item() >= 0.5: count_fn_corrected += 1\n        total_fn += 1\n    print(f\"FNs: Intervened on vanilla_concept_idx {int_idx_in_vanilla} with Mean: Corrections: {count_fn_corrected/total_fn if total_fn > 0 else 'N/A'} ({count_fn_corrected}/{total_fn})\")\n\n    count_fp_corrected, total_fp = 0, 0\n    for a in fp_idx_m1_seq:\n        intervened_vanilla_concepts = c_pred_m1_seq[a].copy()\n        intervened_vanilla_concepts[int_idx_in_vanilla] = mean_val_for_concept\n        continuous_slice_for_int_model = intervened_vanilla_concepts[continuous_intervention_indices_in_vanilla]\n        with torch.no_grad():\n            p_int = model_int2_seq(torch.tensor(test_features[a:a+1]).float().to(device),\n                                   torch.tensor([continuous_slice_for_int_model]).float().to(device))\n        if p_int.item() < 0.5: count_fp_corrected += 1\n        total_fp += 1\n    print(f\"FPs: Intervened on vanilla_concept_idx {int_idx_in_vanilla} with Mean: Corrections: {count_fp_corrected/total_fp if total_fp > 0 else 'N/A'} ({count_fp_corrected}/{total_fp})\")\n\n# Intervention Type: Replacement with Median Values\nprint(\"\\n--- Continuous Concepts: Replacement with Median Values (model1_sequential) ---\")\nmedian_true_continuous_concepts = np.median(test_concepts[:, continuous_intervention_indices_in_vanilla], axis=0)\nfor i, int_idx_in_vanilla in enumerate(continuous_intervention_indices_in_vanilla):\n    median_val_for_concept = median_true_continuous_concepts[i]\n    count_fn_corrected, total_fn = 0, 0\n    for a in fn_idx_m1_seq:\n        intervened_vanilla_concepts = c_pred_m1_seq[a].copy()\n        intervened_vanilla_concepts[int_idx_in_vanilla] = median_val_for_concept\n        continuous_slice_for_int_model = intervened_vanilla_concepts[continuous_intervention_indices_in_vanilla]\n        with torch.no_grad():\n            p_int = model_int2_seq(torch.tensor(test_features[a:a+1]).float().to(device),\n                                   torch.tensor([continuous_slice_for_int_model]).float().to(device))\n        if p_int.item() >= 0.5: count_fn_corrected += 1\n        total_fn += 1\n    print(f\"FNs: Intervened on vanilla_concept_idx {int_idx_in_vanilla} with Median: Corrections: {count_fn_corrected/total_fn if total_fn > 0 else 'N/A'} ({count_fn_corrected}/{total_fn})\")\n\n    count_fp_corrected, total_fp = 0, 0\n    for a in fp_idx_m1_seq:\n        intervened_vanilla_concepts = c_pred_m1_seq[a].copy()\n        intervened_vanilla_concepts[int_idx_in_vanilla] = median_val_for_concept\n        continuous_slice_for_int_model = intervened_vanilla_concepts[continuous_intervention_indices_in_vanilla]\n        with torch.no_grad():\n            p_int = model_int2_seq(torch.tensor(test_features[a:a+1]).float().to(device),\n                                   torch.tensor([continuous_slice_for_int_model]).float().to(device))\n        if p_int.item() < 0.5: count_fp_corrected += 1\n        total_fp += 1\n    print(f\"FPs: Intervened on vanilla_concept_idx {int_idx_in_vanilla} with Median: Corrections: {count_fp_corrected/total_fp if total_fp > 0 else 'N/A'} ({count_fp_corrected}/{total_fp})\")\n\n\nprint(\"\\n###########################################################################\")\nprint(\"--- Interventions on Enhanced CBM (model2_sequential) ---\")\nprint(\"###########################################################################\")\n\n# --- Binary Concepts Interventions for model2_sequential (Adapted from Cells 35, 36, 37) ---\n# Using model_int3_seq\n# FN/FP indices are fn_idx_m2_seq, fp_idx_m2_seq\n# Predicted concepts c_pred_m2_seq\n# True concepts test_concepts, test_llm_concepts\n\n# Intervention Type: Replacement with True Values\nprint(\"\\n--- Binary Concepts: Replacement with True Values (model2_sequential) ---\")\nfor int_idx_in_vanilla in binary_intervention_indices_in_vanilla:\n    count_fn_corrected, total_fn = 0, 0\n    for a in fn_idx_m2_seq:\n        intervened_vanilla_concepts = c_pred_m2_seq[a].copy()\n        intervened_vanilla_concepts[int_idx_in_vanilla] = test_concepts[a][int_idx_in_vanilla]\n        binary_slice_for_int_model = intervened_vanilla_concepts[binary_intervention_indices_in_vanilla]\n        with torch.no_grad():\n            p_int = model_int3_seq(torch.tensor(test_features[a:a+1]).float().to(device),\n                                   torch.tensor([binary_slice_for_int_model]).float().to(device),\n                                   torch.tensor(test_llm_concepts[a:a+1]).float().to(device))\n        if p_int.item() >= 0.5: count_fn_corrected += 1\n        total_fn += 1\n    print(f\"FNs: Intervened on vanilla_concept_idx {int_idx_in_vanilla}: Corrections: {count_fn_corrected/total_fn if total_fn > 0 else 'N/A'} ({count_fn_corrected}/{total_fn})\")\n    \n    count_fp_corrected, total_fp = 0, 0\n    for a in fp_idx_m2_seq:\n        intervened_vanilla_concepts = c_pred_m2_seq[a].copy()\n        intervened_vanilla_concepts[int_idx_in_vanilla] = test_concepts[a][int_idx_in_vanilla]\n        binary_slice_for_int_model = intervened_vanilla_concepts[binary_intervention_indices_in_vanilla]\n        with torch.no_grad():\n            p_int = model_int3_seq(torch.tensor(test_features[a:a+1]).float().to(device),\n                                   torch.tensor([binary_slice_for_int_model]).float().to(device),\n                                   torch.tensor(test_llm_concepts[a:a+1]).float().to(device))\n        if p_int.item() <= 0.5: count_fp_corrected += 1 # Note: original notebook had <=0.5 for FP correction here\n        total_fp += 1\n    print(f\"FPs: Intervened on vanilla_concept_idx {int_idx_in_vanilla}: Corrections: {count_fp_corrected/total_fp if total_fp > 0 else 'N/A'} ({count_fp_corrected}/{total_fp})\")\n\n# All binary concepts together with true values\ncount_fn_corrected, total_fn = 0, 0\nfor a in fn_idx_m2_seq:\n    intervened_vanilla_concepts = c_pred_m2_seq[a].copy()\n    for bc_idx in binary_intervention_indices_in_vanilla:\n        intervened_vanilla_concepts[bc_idx] = test_concepts[a][bc_idx]\n    binary_slice_for_int_model = intervened_vanilla_concepts[binary_intervention_indices_in_vanilla]\n    with torch.no_grad():\n        p_int = model_int3_seq(torch.tensor(test_features[a:a+1]).float().to(device),\n                               torch.tensor([binary_slice_for_int_model]).float().to(device),\n                               torch.tensor(test_llm_concepts[a:a+1]).float().to(device))\n    if p_int.item() >= 0.5: count_fn_corrected += 1\n    total_fn += 1\nprint(f\"FNs: All Binary Concepts (True): Corrections: {count_fn_corrected/total_fn if total_fn > 0 else 'N/A'} ({count_fn_corrected}/{total_fn})\")\n\ncount_fp_corrected, total_fp = 0, 0\nfor a in fp_idx_m2_seq:\n    intervened_vanilla_concepts = c_pred_m2_seq[a].copy()\n    for bc_idx in binary_intervention_indices_in_vanilla:\n        intervened_vanilla_concepts[bc_idx] = test_concepts[a][bc_idx]\n    binary_slice_for_int_model = intervened_vanilla_concepts[binary_intervention_indices_in_vanilla]\n    with torch.no_grad():\n        p_int = model_int3_seq(torch.tensor(test_features[a:a+1]).float().to(device),\n                               torch.tensor([binary_slice_for_int_model]).float().to(device),\n                               torch.tensor(test_llm_concepts[a:a+1]).float().to(device))\n    if p_int.item() <= 0.5: count_fp_corrected += 1\n    total_fp += 1\nprint(f\"FPs: All Binary Concepts (True): Corrections: {count_fp_corrected/total_fp if total_fp > 0 else 'N/A'} ({count_fp_corrected}/{total_fp})\")\n\n\n# Intervention Type: Replacement with Mean Values\nprint(\"\\n--- Binary Concepts: Replacement with Mean Values (model2_sequential) ---\")\n# mean_true_binary_concepts is already calculated\nfor i, int_idx_in_vanilla in enumerate(binary_intervention_indices_in_vanilla):\n    mean_val_for_concept = mean_true_binary_concepts[i]\n    count_fn_corrected, total_fn = 0, 0\n    for a in fn_idx_m2_seq:\n        intervened_vanilla_concepts = c_pred_m2_seq[a].copy()\n        intervened_vanilla_concepts[int_idx_in_vanilla] = mean_val_for_concept\n        binary_slice_for_int_model = intervened_vanilla_concepts[binary_intervention_indices_in_vanilla]\n        with torch.no_grad():\n            p_int = model_int3_seq(torch.tensor(test_features[a:a+1]).float().to(device),\n                                   torch.tensor([binary_slice_for_int_model]).float().to(device),\n                                   torch.tensor(test_llm_concepts[a:a+1]).float().to(device))\n        if p_int.item() >= 0.5: count_fn_corrected += 1\n        total_fn += 1\n    print(f\"FNs: Intervened on vanilla_concept_idx {int_idx_in_vanilla} with Mean: Corrections: {count_fn_corrected/total_fn if total_fn > 0 else 'N/A'} ({count_fn_corrected}/{total_fn})\")\n\n    count_fp_corrected, total_fp = 0, 0\n    for a in fp_idx_m2_seq:\n        intervened_vanilla_concepts = c_pred_m2_seq[a].copy()\n        intervened_vanilla_concepts[int_idx_in_vanilla] = mean_val_for_concept\n        binary_slice_for_int_model = intervened_vanilla_concepts[binary_intervention_indices_in_vanilla]\n        with torch.no_grad():\n            p_int = model_int3_seq(torch.tensor(test_features[a:a+1]).float().to(device),\n                                   torch.tensor([binary_slice_for_int_model]).float().to(device),\n                                   torch.tensor(test_llm_concepts[a:a+1]).float().to(device))\n        if p_int.item() <= 0.5: count_fp_corrected += 1\n        total_fp += 1\n    print(f\"FPs: Intervened on vanilla_concept_idx {int_idx_in_vanilla} with Mean: Corrections: {count_fp_corrected/total_fp if total_fp > 0 else 'N/A'} ({count_fp_corrected}/{total_fp})\")\n\n\n# Intervention Type: Replacement with Median Values\nprint(\"\\n--- Binary Concepts: Replacement with Median Values (model2_sequential) ---\")\n# median_true_binary_concepts is already calculated\nfor i, int_idx_in_vanilla in enumerate(binary_intervention_indices_in_vanilla):\n    median_val_for_concept = median_true_binary_concepts[i]\n    count_fn_corrected, total_fn = 0, 0\n    for a in fn_idx_m2_seq:\n        intervened_vanilla_concepts = c_pred_m2_seq[a].copy()\n        intervened_vanilla_concepts[int_idx_in_vanilla] = median_val_for_concept\n        binary_slice_for_int_model = intervened_vanilla_concepts[binary_intervention_indices_in_vanilla]\n        with torch.no_grad():\n            p_int = model_int3_seq(torch.tensor(test_features[a:a+1]).float().to(device),\n                                   torch.tensor([binary_slice_for_int_model]).float().to(device),\n                                   torch.tensor(test_llm_concepts[a:a+1]).float().to(device))\n        if p_int.item() >= 0.5: count_fn_corrected += 1\n        total_fn += 1\n    print(f\"FNs: Intervened on vanilla_concept_idx {int_idx_in_vanilla} with Median: Corrections: {count_fn_corrected/total_fn if total_fn > 0 else 'N/A'} ({count_fn_corrected}/{total_fn})\")\n\n    count_fp_corrected, total_fp = 0, 0\n    for a in fp_idx_m2_seq:\n        intervened_vanilla_concepts = c_pred_m2_seq[a].copy()\n        intervened_vanilla_concepts[int_idx_in_vanilla] = median_val_for_concept\n        binary_slice_for_int_model = intervened_vanilla_concepts[binary_intervention_indices_in_vanilla]\n        with torch.no_grad():\n            p_int = model_int3_seq(torch.tensor(test_features[a:a+1]).float().to(device),\n                                   torch.tensor([binary_slice_for_int_model]).float().to(device),\n                                   torch.tensor(test_llm_concepts[a:a+1]).float().to(device))\n        if p_int.item() <= 0.5: count_fp_corrected += 1\n        total_fp += 1\n    print(f\"FPs: Intervened on vanilla_concept_idx {int_idx_in_vanilla} with Median: Corrections: {count_fp_corrected/total_fp if total_fp > 0 else 'N/A'} ({count_fp_corrected}/{total_fp})\")\n\n\n# --- Continuous Concepts Interventions for model2_sequential (Adapted from Cells 38, 39, 40) ---\n# Using model_int4_seq\n\n# Intervention Type: Replacement with True Values\nprint(\"\\n--- Continuous Concepts: Replacement with True Values (model2_sequential) ---\")\nfor int_idx_in_vanilla in continuous_intervention_indices_in_vanilla: # Indices 0-11\n    count_fn_corrected, total_fn = 0, 0\n    for a in fn_idx_m2_seq:\n        intervened_vanilla_concepts = c_pred_m2_seq[a].copy()\n        intervened_vanilla_concepts[int_idx_in_vanilla] = test_concepts[a][int_idx_in_vanilla]\n        continuous_slice_for_int_model = intervened_vanilla_concepts[continuous_intervention_indices_in_vanilla]\n        with torch.no_grad():\n            p_int = model_int4_seq(torch.tensor(test_features[a:a+1]).float().to(device),\n                                   torch.tensor([continuous_slice_for_int_model]).float().to(device),\n                                   torch.tensor(test_llm_concepts[a:a+1]).float().to(device))\n        if p_int.item() >= 0.5: count_fn_corrected += 1\n        total_fn += 1\n    print(f\"FNs: Intervened on vanilla_concept_idx {int_idx_in_vanilla}: Corrections: {count_fn_corrected/total_fn if total_fn > 0 else 'N/A'} ({count_fn_corrected}/{total_fn})\")\n\n    count_fp_corrected, total_fp = 0, 0\n    for a in fp_idx_m2_seq:\n        intervened_vanilla_concepts = c_pred_m2_seq[a].copy()\n        intervened_vanilla_concepts[int_idx_in_vanilla] = test_concepts[a][int_idx_in_vanilla]\n        continuous_slice_for_int_model = intervened_vanilla_concepts[continuous_intervention_indices_in_vanilla]\n        with torch.no_grad():\n            p_int = model_int4_seq(torch.tensor(test_features[a:a+1]).float().to(device),\n                                   torch.tensor([continuous_slice_for_int_model]).float().to(device),\n                                   torch.tensor(test_llm_concepts[a:a+1]).float().to(device))\n        if p_int.item() <= 0.5: count_fp_corrected += 1\n        total_fp += 1\n    print(f\"FPs: Intervened on vanilla_concept_idx {int_idx_in_vanilla}: Corrections: {count_fp_corrected/total_fp if total_fp > 0 else 'N/A'} ({count_fp_corrected}/{total_fp})\")\n\n# Intervention Type: Replacement with Mean Values\nprint(\"\\n--- Continuous Concepts: Replacement with Mean Values (model2_sequential) ---\")\n# mean_true_continuous_concepts is already calculated\nfor i, int_idx_in_vanilla in enumerate(continuous_intervention_indices_in_vanilla):\n    mean_val_for_concept = mean_true_continuous_concepts[i]\n    count_fn_corrected, total_fn = 0, 0\n    for a in fn_idx_m2_seq:\n        intervened_vanilla_concepts = c_pred_m2_seq[a].copy()\n        intervened_vanilla_concepts[int_idx_in_vanilla] = mean_val_for_concept\n        continuous_slice_for_int_model = intervened_vanilla_concepts[continuous_intervention_indices_in_vanilla]\n        with torch.no_grad():\n            p_int = model_int4_seq(torch.tensor(test_features[a:a+1]).float().to(device),\n                                   torch.tensor([continuous_slice_for_int_model]).float().to(device),\n                                   torch.tensor(test_llm_concepts[a:a+1]).float().to(device))\n        if p_int.item() >= 0.5: count_fn_corrected += 1\n        total_fn += 1\n    print(f\"FNs: Intervened on vanilla_concept_idx {int_idx_in_vanilla} with Mean: Corrections: {count_fn_corrected/total_fn if total_fn > 0 else 'N/A'} ({count_fn_corrected}/{total_fn})\")\n\n    count_fp_corrected, total_fp = 0, 0\n    for a in fp_idx_m2_seq:\n        intervened_vanilla_concepts = c_pred_m2_seq[a].copy()\n        intervened_vanilla_concepts[int_idx_in_vanilla] = mean_val_for_concept\n        continuous_slice_for_int_model = intervened_vanilla_concepts[continuous_intervention_indices_in_vanilla]\n        with torch.no_grad():\n            p_int = model_int4_seq(torch.tensor(test_features[a:a+1]).float().to(device),\n                                   torch.tensor([continuous_slice_for_int_model]).float().to(device),\n                                   torch.tensor(test_llm_concepts[a:a+1]).float().to(device))\n        if p_int.item() <= 0.5: count_fp_corrected += 1\n        total_fp += 1\n    print(f\"FPs: Intervened on vanilla_concept_idx {int_idx_in_vanilla} with Mean: Corrections: {count_fp_corrected/total_fp if total_fp > 0 else 'N/A'} ({count_fp_corrected}/{total_fp})\")\n\n# Intervention Type: Replacement with Median Values\nprint(\"\\n--- Continuous Concepts: Replacement with Median Values (model2_sequential) ---\")\n# median_true_continuous_concepts is already calculated\nfor i, int_idx_in_vanilla in enumerate(continuous_intervention_indices_in_vanilla):\n    median_val_for_concept = median_true_continuous_concepts[i]\n    count_fn_corrected, total_fn = 0, 0\n    for a in fn_idx_m2_seq:\n        intervened_vanilla_concepts = c_pred_m2_seq[a].copy()\n        intervened_vanilla_concepts[int_idx_in_vanilla] = median_val_for_concept\n        continuous_slice_for_int_model = intervened_vanilla_concepts[continuous_intervention_indices_in_vanilla]\n        with torch.no_grad():\n            p_int = model_int4_seq(torch.tensor(test_features[a:a+1]).float().to(device),\n                                   torch.tensor([continuous_slice_for_int_model]).float().to(device),\n                                   torch.tensor(test_llm_concepts[a:a+1]).float().to(device))\n        if p_int.item() >= 0.5: count_fn_corrected += 1\n        total_fn += 1\n    print(f\"FNs: Intervened on vanilla_concept_idx {int_idx_in_vanilla} with Median: Corrections: {count_fn_corrected/total_fn if total_fn > 0 else 'N/A'} ({count_fn_corrected}/{total_fn})\")\n\n    count_fp_corrected, total_fp = 0, 0\n    for a in fp_idx_m2_seq:\n        intervened_vanilla_concepts = c_pred_m2_seq[a].copy()\n        intervened_vanilla_concepts[int_idx_in_vanilla] = median_val_for_concept\n        continuous_slice_for_int_model = intervened_vanilla_concepts[continuous_intervention_indices_in_vanilla]\n        with torch.no_grad():\n            p_int = model_int4_seq(torch.tensor(test_features[a:a+1]).float().to(device),\n                                   torch.tensor([continuous_slice_for_int_model]).float().to(device),\n                                   torch.tensor(test_llm_concepts[a:a+1]).float().to(device))\n        if p_int.item() <= 0.5: count_fp_corrected += 1\n        total_fp += 1\n    print(f\"FPs: Intervened on vanilla_concept_idx {int_idx_in_vanilla} with Median: Corrections: {count_fp_corrected/total_fp if total_fp > 0 else 'N/A'} ({count_fp_corrected}/{total_fp})\")\n\n\n# --- LLM Concepts Interventions for model2_sequential (Adapted from Cells 43, 45, 46) ---\n# Using model_int5_seq\nllm_concept_indices = list(range(num_llm_concepts)) # Indices 0-7 for LLM concepts\n\n# Intervention Type: Manual Changes (1.0 for FN, 0.0 for FP)\nprint(\"\\n--- LLM Concepts: Manual Changes (1.0 for FN, 0.0 for FP) (model2_sequential) ---\")\nfor llm_idx in llm_concept_indices:\n    count_fn_corrected, total_fn = 0, 0\n    for a in fn_idx_m2_seq:\n        intervened_llm = test_llm_concepts[a].copy() # Start with true LLM concepts\n        intervened_llm[llm_idx] = 1.0 # Intervene one LLM concept to 1.0\n        with torch.no_grad():\n            p_int = model_int5_seq(torch.tensor(test_features[a:a+1]).float().to(device),\n                                   torch.tensor([intervened_llm]).float().to(device))\n        if p_int.item() >= 0.5: count_fn_corrected += 1\n        total_fn += 1\n    print(f\"FNs: Intervened on LLM_concept_idx {llm_idx} to 1.0: Corrections: {count_fn_corrected/total_fn if total_fn > 0 else 'N/A'} ({count_fn_corrected}/{total_fn})\")\n\n    count_fp_corrected, total_fp = 0, 0\n    for a in fp_idx_m2_seq:\n        intervened_llm = test_llm_concepts[a].copy()\n        intervened_llm[llm_idx] = 0.0 # Intervene one LLM concept to 0.0\n        with torch.no_grad():\n            p_int = model_int5_seq(torch.tensor(test_features[a:a+1]).float().to(device),\n                                   torch.tensor([intervened_llm]).float().to(device))\n        if p_int.item() <= 0.5: count_fp_corrected += 1\n        total_fp += 1\n    print(f\"FPs: Intervened on LLM_concept_idx {llm_idx} to 0.0: Corrections: {count_fp_corrected/total_fp if total_fp > 0 else 'N/A'} ({count_fp_corrected}/{total_fp})\")\n\n# Example: All specified LLM concepts for FNs changed to 1.0 (from your original Cell 43)\nfn_llm_indices_to_one = [0, 1, 5] # Example from your original notebook\ncount_fn_corrected, total_fn = 0, 0\nfor a in fn_idx_m2_seq:\n    intervened_llm = test_llm_concepts[a].copy()\n    for idx_to_one in fn_llm_indices_to_one:\n        intervened_llm[idx_to_one] = 1.0\n    with torch.no_grad():\n        p_int = model_int5_seq(torch.tensor(test_features[a:a+1]).float().to(device),\n                               torch.tensor([intervened_llm]).float().to(device))\n    if p_int.item() >= 0.5: count_fn_corrected += 1\n    total_fn += 1\nprint(f\"FNs: LLM concepts {fn_llm_indices_to_one} to 1.0: Corrections: {count_fn_corrected/total_fn if total_fn > 0 else 'N/A'} ({count_fn_corrected}/{total_fn})\")\n\n# Example: All specified LLM concepts for FPs changed to 0.0 (from your original Cell 43)\nfp_llm_indices_to_zero = [0, 2, 6] # Example from your original notebook\ncount_fp_corrected, total_fp = 0, 0\nfor a in fp_idx_m2_seq:\n    intervened_llm = test_llm_concepts[a].copy()\n    for idx_to_zero in fp_llm_indices_to_zero:\n        intervened_llm[idx_to_zero] = 0.0\n    with torch.no_grad():\n        p_int = model_int5_seq(torch.tensor(test_features[a:a+1]).float().to(device),\n                               torch.tensor([intervened_llm]).float().to(device))\n    if p_int.item() <= 0.5: count_fp_corrected += 1\n    total_fp += 1\nprint(f\"FPs: LLM concepts {fp_llm_indices_to_zero} to 0.0: Corrections: {count_fp_corrected/total_fp if total_fp > 0 else 'N/A'} ({count_fp_corrected}/{total_fp})\")\n\n\n# Intervention Type: Replacement with Mean Values\nprint(\"\\n--- LLM Concepts: Replacement with Mean Values (model2_sequential) ---\")\nmean_true_llm_concepts = np.mean(test_llm_concepts, axis=0)\nfor i, llm_idx in enumerate(llm_concept_indices):\n    mean_val_for_llm_concept = mean_true_llm_concepts[i]\n    count_fn_corrected, total_fn = 0, 0\n    for a in fn_idx_m2_seq:\n        intervened_llm = test_llm_concepts[a].copy() # Start with true LLM concepts\n        intervened_llm[llm_idx] = mean_val_for_llm_concept\n        with torch.no_grad():\n            p_int = model_int5_seq(torch.tensor(test_features[a:a+1]).float().to(device),\n                                   torch.tensor([intervened_llm]).float().to(device))\n        if p_int.item() >= 0.5: count_fn_corrected += 1\n        total_fn += 1\n    print(f\"FNs: Intervened on LLM_concept_idx {llm_idx} with Mean: Corrections: {count_fn_corrected/total_fn if total_fn > 0 else 'N/A'} ({count_fn_corrected}/{total_fn})\")\n\n    count_fp_corrected, total_fp = 0, 0\n    for a in fp_idx_m2_seq:\n        intervened_llm = test_llm_concepts[a].copy()\n        intervened_llm[llm_idx] = mean_val_for_llm_concept\n        with torch.no_grad():\n            p_int = model_int5_seq(torch.tensor(test_features[a:a+1]).float().to(device),\n                                   torch.tensor([intervened_llm]).float().to(device))\n        if p_int.item() <= 0.5: count_fp_corrected += 1\n        total_fp += 1\n    print(f\"FPs: Intervened on LLM_concept_idx {llm_idx} with Mean: Corrections: {count_fp_corrected/total_fp if total_fp > 0 else 'N/A'} ({count_fp_corrected}/{total_fp})\")\n\n\n# Intervention Type: Replacement with Median Values\nprint(\"\\n--- LLM Concepts: Replacement with Median Values (model2_sequential) ---\")\nmedian_true_llm_concepts = np.median(test_llm_concepts, axis=0)\nfor i, llm_idx in enumerate(llm_concept_indices):\n    median_val_for_llm_concept = median_true_llm_concepts[i]\n    count_fn_corrected, total_fn = 0, 0\n    for a in fn_idx_m2_seq:\n        intervened_llm = test_llm_concepts[a].copy()\n        intervened_llm[llm_idx] = median_val_for_llm_concept\n        with torch.no_grad():\n            p_int = model_int5_seq(torch.tensor(test_features[a:a+1]).float().to(device),\n                                   torch.tensor([intervened_llm]).float().to(device))\n        if p_int.item() >= 0.5: count_fn_corrected += 1\n        total_fn += 1\n    print(f\"FNs: Intervened on LLM_concept_idx {llm_idx} with Median: Corrections: {count_fn_corrected/total_fn if total_fn > 0 else 'N/A'} ({count_fn_corrected}/{total_fn})\")\n\n    count_fp_corrected, total_fp = 0, 0\n    for a in fp_idx_m2_seq:\n        intervened_llm = test_llm_concepts[a].copy()\n        intervened_llm[llm_idx] = median_val_for_llm_concept\n        with torch.no_grad():\n            p_int = model_int5_seq(torch.tensor(test_features[a:a+1]).float().to(device),\n                                   torch.tensor([intervened_llm]).float().to(device))\n        if p_int.item() <= 0.5: count_fp_corrected += 1\n        total_fp += 1\n    print(f\"FPs: Intervened on LLM_concept_idx {llm_idx} with Median: Corrections: {count_fp_corrected/total_fp if total_fp > 0 else 'N/A'} ({count_fp_corrected}/{total_fp})\")\n\n# --- Interventions on all three kinds of concepts simultaneously (Adapted from Cell 48, 49, 50) ---\n# Using model_int6_seq\n\n# Example: Best of ground truth replacements (FNs)\nprint(\"\\n--- All Concepts (GT): Best of Ground Truth Replacements for FNs (model2_sequential) ---\")\ncount_fn_corr_all_gt, total_fn_all_gt = 0, 0\nfor a in fn_idx_m2_seq:\n    # Vanilla concepts part - from c_pred_m2_seq, intervene select with true\n    intervened_vanilla = c_pred_m2_seq[a].copy()\n    intervened_vanilla[12] = test_concepts[a][12] # Example: binary concept 0 (idx 12)\n    intervened_vanilla[7] = test_concepts[a][7]   # Example: continuous concept 7 (idx 7)\n    \n    binary_slice = intervened_vanilla[binary_intervention_indices_in_vanilla]\n    continuous_slice = intervened_vanilla[continuous_intervention_indices_in_vanilla]\n\n    # LLM concepts part - from test_llm_concepts, intervene select manually\n    intervened_llm = test_llm_concepts[a].copy()\n    intervened_llm[0] = 1.0 # Example: LLM concept 0 to 1.0\n    intervened_llm[1] = 1.0 # Example: LLM concept 1 to 1.0\n    intervened_llm[5] = 1.0 # Example: LLM concept 5 to 1.0\n\n    with torch.no_grad():\n        p_int = model_int6_seq(\n            torch.tensor(test_features[a:a+1]).float().to(device),\n            torch.tensor([binary_slice]).float().to(device),\n            torch.tensor([continuous_slice]).float().to(device),\n            torch.tensor([intervened_llm]).float().to(device)\n        )\n    if p_int.item() >= 0.5: count_fn_corr_all_gt += 1\n    total_fn_all_gt += 1\nprint(f\"FNs: All Concepts (Selected GT & Manual LLM): Corrections: {count_fn_corr_all_gt/total_fn_all_gt if total_fn_all_gt > 0 else 'N/A'} ({count_fn_corr_all_gt}/{total_fn_all_gt})\")\n\n# Example: Best of ground truth replacements (FPs)\nprint(\"\\n--- All Concepts (GT): Best of Ground Truth Replacements for FPs (model2_sequential) ---\")\ncount_fp_corr_all_gt, total_fp_all_gt = 0, 0\nfor a in fp_idx_m2_seq:\n    intervened_vanilla = c_pred_m2_seq[a].copy()\n    intervened_vanilla[12] = test_concepts[a][12] # Example\n    intervened_vanilla[5] = test_concepts[a][5]   # Example\n    intervened_vanilla[6] = test_concepts[a][6]   # Example\n    intervened_vanilla[9] = test_concepts[a][9]   # Example\n    \n    binary_slice = intervened_vanilla[binary_intervention_indices_in_vanilla]\n    continuous_slice = intervened_vanilla[continuous_intervention_indices_in_vanilla]\n\n    intervened_llm = test_llm_concepts[a].copy()\n    intervened_llm[0] = 0.0 # Example\n    intervened_llm[2] = 0.0 # Example\n    intervened_llm[6] = 0.0 # Example\n\n    with torch.no_grad():\n        p_int = model_int6_seq(\n            torch.tensor(test_features[a:a+1]).float().to(device),\n            torch.tensor([binary_slice]).float().to(device),\n            torch.tensor([continuous_slice]).float().to(device),\n            torch.tensor([intervened_llm]).float().to(device)\n        )\n    if p_int.item() < 0.5: count_fp_corr_all_gt += 1\n    total_fp_all_gt += 1\nprint(f\"FPs: All Concepts (Selected GT & Manual LLM): Corrections: {count_fp_corr_all_gt/total_fp_all_gt if total_fp_all_gt > 0 else 'N/A'} ({count_fp_corr_all_gt}/{total_fp_all_gt})\")\n\n# ... Continue adapting for mean and median replacements for all concepts, similar to your Cells 49 and 50 ...","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T06:26:38.788596Z","iopub.execute_input":"2025-05-29T06:26:38.788950Z","iopub.status.idle":"2025-05-29T06:26:40.281308Z","shell.execute_reply.started":"2025-05-29T06:26:38.788923Z","shell.execute_reply":"2025-05-29T06:26:40.280515Z"}},"outputs":[{"name":"stdout","text":"\n###########################################################################\n--- Interventions on Base CBM (model1_sequential) ---\n###########################################################################\n\n--- Binary Concepts: Replacement with True Values (model1_sequential) ---\nFNs: Intervened on vanilla_concept_idx 12: Corrections: 0.0 (0/55)\nFPs: Intervened on vanilla_concept_idx 12: Corrections: 0.11428571428571428 (8/70)\nFNs: Intervened on vanilla_concept_idx 13: Corrections: 0.01818181818181818 (1/55)\nFPs: Intervened on vanilla_concept_idx 13: Corrections: 0.014285714285714285 (1/70)\nFNs: All Binary Concepts (True): Corrections: 0.0 (0/55)\nFPs: All Binary Concepts (True): Corrections: 0.12857142857142856 (9/70)\n\n--- Binary Concepts: Replacement with Mean Values (model1_sequential) ---\nFNs: Intervened on vanilla_concept_idx 12 with Mean: Corrections: 0.38181818181818183 (21/55)\nFPs: Intervened on vanilla_concept_idx 12 with Mean: Corrections: 0.07142857142857142 (5/70)\nFNs: Intervened on vanilla_concept_idx 13 with Mean: Corrections: 0.0 (0/55)\nFPs: Intervened on vanilla_concept_idx 13 with Mean: Corrections: 0.02857142857142857 (2/70)\nFNs: All Binary Concepts (Mean): Corrections: 0.36363636363636365 (20/55)\nFPs: All Binary Concepts (Mean): Corrections: 0.07142857142857142 (5/70)\n\n--- Binary Concepts: Replacement with Median Values (model1_sequential) ---\nFNs: Intervened on vanilla_concept_idx 12 with Median: Corrections: 0.0 (0/55)\nFPs: Intervened on vanilla_concept_idx 12 with Median: Corrections: 0.2571428571428571 (18/70)\nFNs: Intervened on vanilla_concept_idx 13 with Median: Corrections: 0.01818181818181818 (1/55)\nFPs: Intervened on vanilla_concept_idx 13 with Median: Corrections: 0.0 (0/70)\nFNs: All Binary Concepts (Median): Corrections: 0.0 (0/55)\nFPs: All Binary Concepts (Median): Corrections: 0.2571428571428571 (18/70)\n\n--- Continuous Concepts: Replacement with True Values (model1_sequential) ---\nFNs: Intervened on vanilla_concept_idx 0: Corrections: 0.03636363636363636 (2/55)\nFPs: Intervened on vanilla_concept_idx 0: Corrections: 0.07142857142857142 (5/70)\nFNs: Intervened on vanilla_concept_idx 1: Corrections: 0.05454545454545454 (3/55)\nFPs: Intervened on vanilla_concept_idx 1: Corrections: 0.02857142857142857 (2/70)\nFNs: Intervened on vanilla_concept_idx 2: Corrections: 0.09090909090909091 (5/55)\nFPs: Intervened on vanilla_concept_idx 2: Corrections: 0.05714285714285714 (4/70)\nFNs: Intervened on vanilla_concept_idx 3: Corrections: 0.01818181818181818 (1/55)\nFPs: Intervened on vanilla_concept_idx 3: Corrections: 0.014285714285714285 (1/70)\nFNs: Intervened on vanilla_concept_idx 4: Corrections: 0.2909090909090909 (16/55)\nFPs: Intervened on vanilla_concept_idx 4: Corrections: 0.17142857142857143 (12/70)\nFNs: Intervened on vanilla_concept_idx 5: Corrections: 0.16363636363636364 (9/55)\nFPs: Intervened on vanilla_concept_idx 5: Corrections: 0.4857142857142857 (34/70)\nFNs: Intervened on vanilla_concept_idx 6: Corrections: 0.21818181818181817 (12/55)\nFPs: Intervened on vanilla_concept_idx 6: Corrections: 0.08571428571428572 (6/70)\nFNs: Intervened on vanilla_concept_idx 7: Corrections: 0.2727272727272727 (15/55)\nFPs: Intervened on vanilla_concept_idx 7: Corrections: 0.2714285714285714 (19/70)\nFNs: Intervened on vanilla_concept_idx 8: Corrections: 0.03636363636363636 (2/55)\nFPs: Intervened on vanilla_concept_idx 8: Corrections: 0.07142857142857142 (5/70)\nFNs: Intervened on vanilla_concept_idx 9: Corrections: 0.09090909090909091 (5/55)\nFPs: Intervened on vanilla_concept_idx 9: Corrections: 0.18571428571428572 (13/70)\nFNs: Intervened on vanilla_concept_idx 10: Corrections: 0.07272727272727272 (4/55)\nFPs: Intervened on vanilla_concept_idx 10: Corrections: 0.18571428571428572 (13/70)\nFNs: Intervened on vanilla_concept_idx 11: Corrections: 0.03636363636363636 (2/55)\nFPs: Intervened on vanilla_concept_idx 11: Corrections: 0.04285714285714286 (3/70)\nFNs: Selected Continuous Concepts [2, 3, 5, 10] (True): Corrections: 0.14545454545454545 (8/55)\nFPs: Selected Continuous Concepts [1, 2, 11] (True): Corrections: 0.11428571428571428 (8/70)\n\n--- Continuous Concepts: Replacement with Mean Values (model1_sequential) ---\nFNs: Intervened on vanilla_concept_idx 0 with Mean: Corrections: 0.01818181818181818 (1/55)\nFPs: Intervened on vanilla_concept_idx 0 with Mean: Corrections: 0.18571428571428572 (13/70)\nFNs: Intervened on vanilla_concept_idx 1 with Mean: Corrections: 0.18181818181818182 (10/55)\nFPs: Intervened on vanilla_concept_idx 1 with Mean: Corrections: 0.1 (7/70)\nFNs: Intervened on vanilla_concept_idx 2 with Mean: Corrections: 0.16363636363636364 (9/55)\nFPs: Intervened on vanilla_concept_idx 2 with Mean: Corrections: 0.1 (7/70)\nFNs: Intervened on vanilla_concept_idx 3 with Mean: Corrections: 0.03636363636363636 (2/55)\nFPs: Intervened on vanilla_concept_idx 3 with Mean: Corrections: 0.014285714285714285 (1/70)\nFNs: Intervened on vanilla_concept_idx 4 with Mean: Corrections: 0.12727272727272726 (7/55)\nFPs: Intervened on vanilla_concept_idx 4 with Mean: Corrections: 0.12857142857142856 (9/70)\nFNs: Intervened on vanilla_concept_idx 5 with Mean: Corrections: 0.01818181818181818 (1/55)\nFPs: Intervened on vanilla_concept_idx 5 with Mean: Corrections: 0.38571428571428573 (27/70)\nFNs: Intervened on vanilla_concept_idx 6 with Mean: Corrections: 0.23636363636363636 (13/55)\nFPs: Intervened on vanilla_concept_idx 6 with Mean: Corrections: 0.05714285714285714 (4/70)\nFNs: Intervened on vanilla_concept_idx 7 with Mean: Corrections: 0.09090909090909091 (5/55)\nFPs: Intervened on vanilla_concept_idx 7 with Mean: Corrections: 0.11428571428571428 (8/70)\nFNs: Intervened on vanilla_concept_idx 8 with Mean: Corrections: 0.0 (0/55)\nFPs: Intervened on vanilla_concept_idx 8 with Mean: Corrections: 0.1 (7/70)\nFNs: Intervened on vanilla_concept_idx 9 with Mean: Corrections: 0.16363636363636364 (9/55)\nFPs: Intervened on vanilla_concept_idx 9 with Mean: Corrections: 0.15714285714285714 (11/70)\nFNs: Intervened on vanilla_concept_idx 10 with Mean: Corrections: 0.14545454545454545 (8/55)\nFPs: Intervened on vanilla_concept_idx 10 with Mean: Corrections: 0.12857142857142856 (9/70)\nFNs: Intervened on vanilla_concept_idx 11 with Mean: Corrections: 0.2 (11/55)\nFPs: Intervened on vanilla_concept_idx 11 with Mean: Corrections: 0.15714285714285714 (11/70)\n\n--- Continuous Concepts: Replacement with Median Values (model1_sequential) ---\nFNs: Intervened on vanilla_concept_idx 0 with Median: Corrections: 0.05454545454545454 (3/55)\nFPs: Intervened on vanilla_concept_idx 0 with Median: Corrections: 0.11428571428571428 (8/70)\nFNs: Intervened on vanilla_concept_idx 1 with Median: Corrections: 0.18181818181818182 (10/55)\nFPs: Intervened on vanilla_concept_idx 1 with Median: Corrections: 0.1 (7/70)\nFNs: Intervened on vanilla_concept_idx 2 with Median: Corrections: 0.2545454545454545 (14/55)\nFPs: Intervened on vanilla_concept_idx 2 with Median: Corrections: 0.05714285714285714 (4/70)\nFNs: Intervened on vanilla_concept_idx 3 with Median: Corrections: 0.03636363636363636 (2/55)\nFPs: Intervened on vanilla_concept_idx 3 with Median: Corrections: 0.014285714285714285 (1/70)\nFNs: Intervened on vanilla_concept_idx 4 with Median: Corrections: 0.6909090909090909 (38/55)\nFPs: Intervened on vanilla_concept_idx 4 with Median: Corrections: 0.0 (0/70)\nFNs: Intervened on vanilla_concept_idx 5 with Median: Corrections: 0.0 (0/55)\nFPs: Intervened on vanilla_concept_idx 5 with Median: Corrections: 0.6285714285714286 (44/70)\nFNs: Intervened on vanilla_concept_idx 6 with Median: Corrections: 0.2545454545454545 (14/55)\nFPs: Intervened on vanilla_concept_idx 6 with Median: Corrections: 0.05714285714285714 (4/70)\nFNs: Intervened on vanilla_concept_idx 7 with Median: Corrections: 0.4 (22/55)\nFPs: Intervened on vanilla_concept_idx 7 with Median: Corrections: 0.0 (0/70)\nFNs: Intervened on vanilla_concept_idx 8 with Median: Corrections: 0.0 (0/55)\nFPs: Intervened on vanilla_concept_idx 8 with Median: Corrections: 0.21428571428571427 (15/70)\nFNs: Intervened on vanilla_concept_idx 9 with Median: Corrections: 0.01818181818181818 (1/55)\nFPs: Intervened on vanilla_concept_idx 9 with Median: Corrections: 0.4 (28/70)\nFNs: Intervened on vanilla_concept_idx 10 with Median: Corrections: 0.14545454545454545 (8/55)\nFPs: Intervened on vanilla_concept_idx 10 with Median: Corrections: 0.12857142857142856 (9/70)\nFNs: Intervened on vanilla_concept_idx 11 with Median: Corrections: 0.2545454545454545 (14/55)\nFPs: Intervened on vanilla_concept_idx 11 with Median: Corrections: 0.1 (7/70)\n\n###########################################################################\n--- Interventions on Enhanced CBM (model2_sequential) ---\n###########################################################################\n\n--- Binary Concepts: Replacement with True Values (model2_sequential) ---\nFNs: Intervened on vanilla_concept_idx 12: Corrections: 0.037037037037037035 (1/27)\nFPs: Intervened on vanilla_concept_idx 12: Corrections: 0.0 (0/53)\nFNs: Intervened on vanilla_concept_idx 13: Corrections: 0.1111111111111111 (3/27)\nFPs: Intervened on vanilla_concept_idx 13: Corrections: 0.05660377358490566 (3/53)\nFNs: All Binary Concepts (True): Corrections: 0.14814814814814814 (4/27)\nFPs: All Binary Concepts (True): Corrections: 0.05660377358490566 (3/53)\n\n--- Binary Concepts: Replacement with Mean Values (model2_sequential) ---\nFNs: Intervened on vanilla_concept_idx 12 with Mean: Corrections: 0.037037037037037035 (1/27)\nFPs: Intervened on vanilla_concept_idx 12 with Mean: Corrections: 0.03773584905660377 (2/53)\nFNs: Intervened on vanilla_concept_idx 13 with Mean: Corrections: 0.07407407407407407 (2/27)\nFPs: Intervened on vanilla_concept_idx 13 with Mean: Corrections: 0.05660377358490566 (3/53)\n\n--- Binary Concepts: Replacement with Median Values (model2_sequential) ---\nFNs: Intervened on vanilla_concept_idx 12 with Median: Corrections: 0.0 (0/27)\nFPs: Intervened on vanilla_concept_idx 12 with Median: Corrections: 0.03773584905660377 (2/53)\nFNs: Intervened on vanilla_concept_idx 13 with Median: Corrections: 0.0 (0/27)\nFPs: Intervened on vanilla_concept_idx 13 with Median: Corrections: 0.1509433962264151 (8/53)\n\n--- Continuous Concepts: Replacement with True Values (model2_sequential) ---\nFNs: Intervened on vanilla_concept_idx 0: Corrections: 0.0 (0/27)\nFPs: Intervened on vanilla_concept_idx 0: Corrections: 0.03773584905660377 (2/53)\nFNs: Intervened on vanilla_concept_idx 1: Corrections: 0.037037037037037035 (1/27)\nFPs: Intervened on vanilla_concept_idx 1: Corrections: 0.018867924528301886 (1/53)\nFNs: Intervened on vanilla_concept_idx 2: Corrections: 0.0 (0/27)\nFPs: Intervened on vanilla_concept_idx 2: Corrections: 0.0 (0/53)\nFNs: Intervened on vanilla_concept_idx 3: Corrections: 0.0 (0/27)\nFPs: Intervened on vanilla_concept_idx 3: Corrections: 0.0 (0/53)\nFNs: Intervened on vanilla_concept_idx 4: Corrections: 0.07407407407407407 (2/27)\nFPs: Intervened on vanilla_concept_idx 4: Corrections: 0.07547169811320754 (4/53)\nFNs: Intervened on vanilla_concept_idx 5: Corrections: 0.2222222222222222 (6/27)\nFPs: Intervened on vanilla_concept_idx 5: Corrections: 0.1509433962264151 (8/53)\nFNs: Intervened on vanilla_concept_idx 6: Corrections: 0.0 (0/27)\nFPs: Intervened on vanilla_concept_idx 6: Corrections: 0.0 (0/53)\nFNs: Intervened on vanilla_concept_idx 7: Corrections: 0.037037037037037035 (1/27)\nFPs: Intervened on vanilla_concept_idx 7: Corrections: 0.018867924528301886 (1/53)\nFNs: Intervened on vanilla_concept_idx 8: Corrections: 0.18518518518518517 (5/27)\nFPs: Intervened on vanilla_concept_idx 8: Corrections: 0.018867924528301886 (1/53)\nFNs: Intervened on vanilla_concept_idx 9: Corrections: 0.07407407407407407 (2/27)\nFPs: Intervened on vanilla_concept_idx 9: Corrections: 0.03773584905660377 (2/53)\nFNs: Intervened on vanilla_concept_idx 10: Corrections: 0.0 (0/27)\nFPs: Intervened on vanilla_concept_idx 10: Corrections: 0.018867924528301886 (1/53)\nFNs: Intervened on vanilla_concept_idx 11: Corrections: 0.0 (0/27)\nFPs: Intervened on vanilla_concept_idx 11: Corrections: 0.0 (0/53)\n\n--- Continuous Concepts: Replacement with Mean Values (model2_sequential) ---\nFNs: Intervened on vanilla_concept_idx 0 with Mean: Corrections: 0.0 (0/27)\nFPs: Intervened on vanilla_concept_idx 0 with Mean: Corrections: 0.018867924528301886 (1/53)\nFNs: Intervened on vanilla_concept_idx 1 with Mean: Corrections: 0.0 (0/27)\nFPs: Intervened on vanilla_concept_idx 1 with Mean: Corrections: 0.0 (0/53)\nFNs: Intervened on vanilla_concept_idx 2 with Mean: Corrections: 0.0 (0/27)\nFPs: Intervened on vanilla_concept_idx 2 with Mean: Corrections: 0.03773584905660377 (2/53)\nFNs: Intervened on vanilla_concept_idx 3 with Mean: Corrections: 0.0 (0/27)\nFPs: Intervened on vanilla_concept_idx 3 with Mean: Corrections: 0.0 (0/53)\nFNs: Intervened on vanilla_concept_idx 4 with Mean: Corrections: 0.07407407407407407 (2/27)\nFPs: Intervened on vanilla_concept_idx 4 with Mean: Corrections: 0.05660377358490566 (3/53)\nFNs: Intervened on vanilla_concept_idx 5 with Mean: Corrections: 0.1111111111111111 (3/27)\nFPs: Intervened on vanilla_concept_idx 5 with Mean: Corrections: 0.11320754716981132 (6/53)\nFNs: Intervened on vanilla_concept_idx 6 with Mean: Corrections: 0.0 (0/27)\nFPs: Intervened on vanilla_concept_idx 6 with Mean: Corrections: 0.018867924528301886 (1/53)\nFNs: Intervened on vanilla_concept_idx 7 with Mean: Corrections: 0.037037037037037035 (1/27)\nFPs: Intervened on vanilla_concept_idx 7 with Mean: Corrections: 0.03773584905660377 (2/53)\nFNs: Intervened on vanilla_concept_idx 8 with Mean: Corrections: 0.07407407407407407 (2/27)\nFPs: Intervened on vanilla_concept_idx 8 with Mean: Corrections: 0.0 (0/53)\nFNs: Intervened on vanilla_concept_idx 9 with Mean: Corrections: 0.14814814814814814 (4/27)\nFPs: Intervened on vanilla_concept_idx 9 with Mean: Corrections: 0.018867924528301886 (1/53)\nFNs: Intervened on vanilla_concept_idx 10 with Mean: Corrections: 0.0 (0/27)\nFPs: Intervened on vanilla_concept_idx 10 with Mean: Corrections: 0.0 (0/53)\nFNs: Intervened on vanilla_concept_idx 11 with Mean: Corrections: 0.037037037037037035 (1/27)\nFPs: Intervened on vanilla_concept_idx 11 with Mean: Corrections: 0.0 (0/53)\n\n--- Continuous Concepts: Replacement with Median Values (model2_sequential) ---\nFNs: Intervened on vanilla_concept_idx 0 with Median: Corrections: 0.0 (0/27)\nFPs: Intervened on vanilla_concept_idx 0 with Median: Corrections: 0.03773584905660377 (2/53)\nFNs: Intervened on vanilla_concept_idx 1 with Median: Corrections: 0.0 (0/27)\nFPs: Intervened on vanilla_concept_idx 1 with Median: Corrections: 0.0 (0/53)\nFNs: Intervened on vanilla_concept_idx 2 with Median: Corrections: 0.0 (0/27)\nFPs: Intervened on vanilla_concept_idx 2 with Median: Corrections: 0.0 (0/53)\nFNs: Intervened on vanilla_concept_idx 3 with Median: Corrections: 0.0 (0/27)\nFPs: Intervened on vanilla_concept_idx 3 with Median: Corrections: 0.0 (0/53)\nFNs: Intervened on vanilla_concept_idx 4 with Median: Corrections: 0.14814814814814814 (4/27)\nFPs: Intervened on vanilla_concept_idx 4 with Median: Corrections: 0.0 (0/53)\nFNs: Intervened on vanilla_concept_idx 5 with Median: Corrections: 0.037037037037037035 (1/27)\nFPs: Intervened on vanilla_concept_idx 5 with Median: Corrections: 0.18867924528301888 (10/53)\nFNs: Intervened on vanilla_concept_idx 6 with Median: Corrections: 0.0 (0/27)\nFPs: Intervened on vanilla_concept_idx 6 with Median: Corrections: 0.0 (0/53)\nFNs: Intervened on vanilla_concept_idx 7 with Median: Corrections: 0.037037037037037035 (1/27)\nFPs: Intervened on vanilla_concept_idx 7 with Median: Corrections: 0.0 (0/53)\nFNs: Intervened on vanilla_concept_idx 8 with Median: Corrections: 0.037037037037037035 (1/27)\nFPs: Intervened on vanilla_concept_idx 8 with Median: Corrections: 0.09433962264150944 (5/53)\nFNs: Intervened on vanilla_concept_idx 9 with Median: Corrections: 0.0 (0/27)\nFPs: Intervened on vanilla_concept_idx 9 with Median: Corrections: 0.09433962264150944 (5/53)\nFNs: Intervened on vanilla_concept_idx 10 with Median: Corrections: 0.0 (0/27)\nFPs: Intervened on vanilla_concept_idx 10 with Median: Corrections: 0.0 (0/53)\nFNs: Intervened on vanilla_concept_idx 11 with Median: Corrections: 0.07407407407407407 (2/27)\nFPs: Intervened on vanilla_concept_idx 11 with Median: Corrections: 0.0 (0/53)\n\n--- LLM Concepts: Manual Changes (1.0 for FN, 0.0 for FP) (model2_sequential) ---\nFNs: Intervened on LLM_concept_idx 0 to 1.0: Corrections: 0.5555555555555556 (15/27)\nFPs: Intervened on LLM_concept_idx 0 to 0.0: Corrections: 0.5849056603773585 (31/53)\nFNs: Intervened on LLM_concept_idx 1 to 1.0: Corrections: 0.48148148148148145 (13/27)\nFPs: Intervened on LLM_concept_idx 1 to 0.0: Corrections: 0.16981132075471697 (9/53)\nFNs: Intervened on LLM_concept_idx 2 to 1.0: Corrections: 0.1111111111111111 (3/27)\nFPs: Intervened on LLM_concept_idx 2 to 0.0: Corrections: 0.39622641509433965 (21/53)\nFNs: Intervened on LLM_concept_idx 3 to 1.0: Corrections: 0.0 (0/27)\nFPs: Intervened on LLM_concept_idx 3 to 0.0: Corrections: 0.0 (0/53)\nFNs: Intervened on LLM_concept_idx 4 to 1.0: Corrections: 0.0 (0/27)\nFPs: Intervened on LLM_concept_idx 4 to 0.0: Corrections: 0.0 (0/53)\nFNs: Intervened on LLM_concept_idx 5 to 1.0: Corrections: 0.4074074074074074 (11/27)\nFPs: Intervened on LLM_concept_idx 5 to 0.0: Corrections: 0.0 (0/53)\nFNs: Intervened on LLM_concept_idx 6 to 1.0: Corrections: 0.25925925925925924 (7/27)\nFPs: Intervened on LLM_concept_idx 6 to 0.0: Corrections: 0.8301886792452831 (44/53)\nFNs: Intervened on LLM_concept_idx 7 to 1.0: Corrections: 0.25925925925925924 (7/27)\nFPs: Intervened on LLM_concept_idx 7 to 0.0: Corrections: 0.018867924528301886 (1/53)\nFNs: LLM concepts [0, 1, 5] to 1.0: Corrections: 1.0 (27/27)\nFPs: LLM concepts [0, 2, 6] to 0.0: Corrections: 1.0 (53/53)\n\n--- LLM Concepts: Replacement with Mean Values (model2_sequential) ---\nFNs: Intervened on LLM_concept_idx 0 with Mean: Corrections: 0.4074074074074074 (11/27)\nFPs: Intervened on LLM_concept_idx 0 with Mean: Corrections: 0.1320754716981132 (7/53)\nFNs: Intervened on LLM_concept_idx 1 with Mean: Corrections: 0.2222222222222222 (6/27)\nFPs: Intervened on LLM_concept_idx 1 with Mean: Corrections: 0.1320754716981132 (7/53)\nFNs: Intervened on LLM_concept_idx 2 with Mean: Corrections: 0.1111111111111111 (3/27)\nFPs: Intervened on LLM_concept_idx 2 with Mean: Corrections: 0.03773584905660377 (2/53)\nFNs: Intervened on LLM_concept_idx 3 with Mean: Corrections: 0.14814814814814814 (4/27)\nFPs: Intervened on LLM_concept_idx 3 with Mean: Corrections: 0.05660377358490566 (3/53)\nFNs: Intervened on LLM_concept_idx 4 with Mean: Corrections: 0.14814814814814814 (4/27)\nFPs: Intervened on LLM_concept_idx 4 with Mean: Corrections: 0.05660377358490566 (3/53)\nFNs: Intervened on LLM_concept_idx 5 with Mean: Corrections: 0.07407407407407407 (2/27)\nFPs: Intervened on LLM_concept_idx 5 with Mean: Corrections: 0.0 (0/53)\nFNs: Intervened on LLM_concept_idx 6 with Mean: Corrections: 0.25925925925925924 (7/27)\nFPs: Intervened on LLM_concept_idx 6 with Mean: Corrections: 0.2641509433962264 (14/53)\nFNs: Intervened on LLM_concept_idx 7 with Mean: Corrections: 0.0 (0/27)\nFPs: Intervened on LLM_concept_idx 7 with Mean: Corrections: 0.018867924528301886 (1/53)\n\n--- LLM Concepts: Replacement with Median Values (model2_sequential) ---\nFNs: Intervened on LLM_concept_idx 0 with Median: Corrections: 0.5555555555555556 (15/27)\nFPs: Intervened on LLM_concept_idx 0 with Median: Corrections: 0.0 (0/53)\nFNs: Intervened on LLM_concept_idx 1 with Median: Corrections: 0.0 (0/27)\nFPs: Intervened on LLM_concept_idx 1 with Median: Corrections: 0.16981132075471697 (9/53)\nFNs: Intervened on LLM_concept_idx 2 with Median: Corrections: 0.1111111111111111 (3/27)\nFPs: Intervened on LLM_concept_idx 2 with Median: Corrections: 0.0 (0/53)\nFNs: Intervened on LLM_concept_idx 3 with Median: Corrections: 0.18518518518518517 (5/27)\nFPs: Intervened on LLM_concept_idx 3 with Median: Corrections: 0.0 (0/53)\nFNs: Intervened on LLM_concept_idx 4 with Median: Corrections: 0.0 (0/27)\nFPs: Intervened on LLM_concept_idx 4 with Median: Corrections: 0.1320754716981132 (7/53)\nFNs: Intervened on LLM_concept_idx 5 with Median: Corrections: 0.0 (0/27)\nFPs: Intervened on LLM_concept_idx 5 with Median: Corrections: 0.0 (0/53)\nFNs: Intervened on LLM_concept_idx 6 with Median: Corrections: 0.25925925925925924 (7/27)\nFPs: Intervened on LLM_concept_idx 6 with Median: Corrections: 0.0 (0/53)\nFNs: Intervened on LLM_concept_idx 7 with Median: Corrections: 0.0 (0/27)\nFPs: Intervened on LLM_concept_idx 7 with Median: Corrections: 0.018867924528301886 (1/53)\n\n--- All Concepts (GT): Best of Ground Truth Replacements for FNs (model2_sequential) ---\nFNs: All Concepts (Selected GT & Manual LLM): Corrections: 1.0 (27/27)\n\n--- All Concepts (GT): Best of Ground Truth Replacements for FPs (model2_sequential) ---\nFPs: All Concepts (Selected GT & Manual LLM): Corrections: 0.9811320754716981 (52/53)\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}